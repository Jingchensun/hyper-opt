{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ecfa63b96582461c8bb09ef7882650c11fd38af380cc5e83709fdd5945a664c6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Data Augmentation For MNIST Dataset\n",
    "\n",
    "This experiment follows Section 5.4 in the [paper](http://proceedings.mlr.press/v108/lorraine20a.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from network import MLP\n",
    "from model import UNetAugmentHyperOptModel\n",
    "from hyper_opt import FixedPointHyperOptimizer\n",
    "from example.data_utils import InfiniteDataLoader, load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some setup parameters\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "base_optimizer = 'SGD'\n",
    "hyper_lr = 0.01 if base_optimizer == 'RMSprop' else 0.01\n",
    "device = \"cuda:5\"\n",
    "\n",
    "device = torch.device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "train_loader, val_loader, test_loader = load_mnist(batch_size=batch_size)\n",
    "train_iter = InfiniteDataLoader(train_loader, device)\n",
    "val_iter = InfiniteDataLoader(val_loader, device)"
   ]
  },
  {
   "source": [
    "Create model and a hyper model which wraps the main model.\n",
    "Here,\n",
    "- main model is just a multi-layer neural network\n",
    "- hyper model is a Unet $f_\\lambda(\\mathbf{x}, \\epsilon)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main model\n",
    "model = MLP(num_layers=5, input_shape=(28, 28))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# hyper model\n",
    "h_model = UNetAugmentHyperOptModel(model, criterion, in_channels=1, num_classes=1, padding=True)\n",
    "h_model.to(device=device)  # h_model contains model so that no need to load model to cuda\n",
    "\n",
    "# optimizer for neural network\n",
    "nn_optimizer = Adam(h_model.parameters, lr=lr)\n",
    "hyper_optimizer = FixedPointHyperOptimizer(\n",
    "    h_model.parameters,\n",
    "    h_model.hyper_parameters,\n",
    "    base_optimizer=base_optimizer,\n",
    "    default=dict(lr=hyper_lr, momentum=0.9),\n",
    "    use_gauss_newton=True,\n",
    "    stochastic=True)\n",
    "hyper_optimizer.set_kwargs(inner_lr=lr, K=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate function on test set\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, correct = 0., 0\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logit = model(x)\n",
    "            loss = criterion(logit, y)\n",
    "            total_loss += loss.item()\n",
    "            pred = torch.argmax(logit, dim=1).float()\n",
    "            correct += (y == pred).float().sum().item()\n",
    "    model.train()\n",
    "    acc = float(correct) / len(test_loader.dataset)\n",
    "    return acc, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs=20, n_warmup_epochs=1, log_freq=20):\n",
    "\n",
    "    T = 10\n",
    "    counter = 0\n",
    "\n",
    "    # a closure computing train loss. Data will be drawn stochastically\n",
    "    def train_loss_func():\n",
    "        x, y = train_iter.next_batch()\n",
    "        train_loss, train_logit = h_model.train_loss(x, y)\n",
    "        return train_loss, train_logit\n",
    "    \n",
    "    while train_iter.epoch_elapsed <= n_epochs:\n",
    "        for _ in range(T):\n",
    "            model.train()\n",
    "            train_x, train_y = train_iter.next_batch()\n",
    "            train_loss, _ = h_model.train_loss(train_x, train_y)\n",
    "            nn_optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            nn_optimizer.step()\n",
    "        \n",
    "        val_x, val_y = val_iter.next_batch()\n",
    "        val_loss = h_model.validation_loss(val_x, val_y)\n",
    "\n",
    "        if train_iter.epoch_elapsed > n_warmup_epochs:\n",
    "            hyper_optimizer.step(train_loss_func, val_loss, verbose=False)\n",
    "        \n",
    "\n",
    "        # print \n",
    "        if counter % 10 == 0 and counter > 0:\n",
    "            eval_acc, eval_loss = evaluate()\n",
    "            train_loss, val_loss = train_loss.item(), val_loss.item()\n",
    "            print(f\"Iter {counter:5d} | train loss {train_loss:5.2f} | val loss {val_loss:5.2f} | test loss {eval_loss:5.2f} | test acc {eval_acc:2.4f}\")\n",
    "\n",
    "        # counter increment\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iter    10 | train loss  0.30 | val loss  0.18 | test loss 24.30 | test acc 0.9115\n",
      "Iter    20 | train loss  0.21 | val loss  0.20 | test loss 19.72 | test acc 0.9302\n",
      "Iter    30 | train loss  0.30 | val loss  0.09 | test loss 17.47 | test acc 0.9360\n",
      "Iter    40 | train loss  0.18 | val loss  0.16 | test loss 16.01 | test acc 0.9475\n",
      "Iter    50 | train loss  0.10 | val loss  0.16 | test loss 13.38 | test acc 0.9541\n",
      "Iter    60 | train loss  0.17 | val loss  0.17 | test loss 14.97 | test acc 0.9461\n",
      "Iter    70 | train loss  0.41 | val loss  0.20 | test loss 14.37 | test acc 0.9494\n",
      "Iter    80 | train loss  0.20 | val loss  0.13 | test loss 13.20 | test acc 0.9566\n",
      "Iter    90 | train loss  0.16 | val loss  0.15 | test loss 17.56 | test acc 0.9437\n",
      "Iter   100 | train loss  0.20 | val loss  0.13 | test loss 13.40 | test acc 0.9524\n",
      "Iter   110 | train loss  0.27 | val loss  0.36 | test loss 13.68 | test acc 0.9570\n",
      "Iter   120 | train loss  0.18 | val loss  0.20 | test loss 14.83 | test acc 0.9520\n",
      "Iter   130 | train loss  0.07 | val loss  0.21 | test loss 14.51 | test acc 0.9558\n",
      "Iter   140 | train loss  0.13 | val loss  0.46 | test loss 16.08 | test acc 0.9516\n",
      "Iter   150 | train loss  0.07 | val loss  0.08 | test loss 14.30 | test acc 0.9545\n",
      "Iter   160 | train loss  0.15 | val loss  0.15 | test loss 13.49 | test acc 0.9589\n",
      "Iter   170 | train loss  0.21 | val loss  0.16 | test loss 13.13 | test acc 0.9604\n",
      "Iter   180 | train loss  0.08 | val loss  0.13 | test loss 13.13 | test acc 0.9580\n",
      "Iter   190 | train loss  0.12 | val loss  0.11 | test loss 13.91 | test acc 0.9544\n",
      "Iter   200 | train loss  0.15 | val loss  0.32 | test loss 11.40 | test acc 0.9621\n",
      "Iter   210 | train loss  0.04 | val loss  0.14 | test loss 14.70 | test acc 0.9567\n",
      "Iter   220 | train loss  0.08 | val loss  0.05 | test loss 14.16 | test acc 0.9604\n",
      "Iter   230 | train loss  0.09 | val loss  0.08 | test loss 13.65 | test acc 0.9573\n",
      "Iter   240 | train loss  0.05 | val loss  0.12 | test loss 12.91 | test acc 0.9589\n",
      "Iter   250 | train loss  0.13 | val loss  0.31 | test loss 15.16 | test acc 0.9592\n",
      "Iter   260 | train loss  0.16 | val loss  0.12 | test loss 13.30 | test acc 0.9572\n",
      "Iter   270 | train loss  0.13 | val loss  0.16 | test loss 12.58 | test acc 0.9644\n",
      "Iter   280 | train loss  0.08 | val loss  0.16 | test loss 12.80 | test acc 0.9629\n",
      "Iter   290 | train loss  0.16 | val loss  0.22 | test loss 12.57 | test acc 0.9616\n",
      "Iter   300 | train loss  0.09 | val loss  0.04 | test loss 10.19 | test acc 0.9681\n",
      "Iter   310 | train loss  0.17 | val loss  0.06 | test loss 11.85 | test acc 0.9655\n",
      "Iter   320 | train loss  0.13 | val loss  0.30 | test loss 13.47 | test acc 0.9586\n",
      "Iter   330 | train loss  0.09 | val loss  0.14 | test loss 13.43 | test acc 0.9630\n",
      "Iter   340 | train loss  0.10 | val loss  0.24 | test loss 15.89 | test acc 0.9560\n",
      "Iter   350 | train loss  0.15 | val loss  0.14 | test loss 11.70 | test acc 0.9672\n",
      "Iter   360 | train loss  0.09 | val loss  0.02 | test loss 13.30 | test acc 0.9641\n",
      "Iter   370 | train loss  0.06 | val loss  0.22 | test loss 13.08 | test acc 0.9643\n",
      "Iter   380 | train loss  0.10 | val loss  0.25 | test loss 11.21 | test acc 0.9650\n",
      "Iter   390 | train loss  0.03 | val loss  0.23 | test loss 13.33 | test acc 0.9638\n",
      "Iter   400 | train loss  0.05 | val loss  0.06 | test loss 12.43 | test acc 0.9674\n",
      "Iter   410 | train loss  0.04 | val loss  0.23 | test loss 14.01 | test acc 0.9676\n",
      "Iter   420 | train loss  0.13 | val loss  0.12 | test loss 11.96 | test acc 0.9665\n",
      "Iter   430 | train loss  0.14 | val loss  0.21 | test loss 13.60 | test acc 0.9662\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "source": [
    "Let's train without augmentation\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iter    20 | train loss  0.54 | test acc:  0.7888\n",
      "Iter    40 | train loss  0.49 | test acc:  0.8765\n",
      "Iter    60 | train loss  0.42 | test acc:  0.9067\n",
      "Iter    80 | train loss  0.39 | test acc:  0.9157\n",
      "Iter   100 | train loss  0.30 | test acc:  0.9104\n",
      "Iter   120 | train loss  0.31 | test acc:  0.9202\n",
      "Iter   140 | train loss  0.25 | test acc:  0.9218\n",
      "Iter   160 | train loss  0.26 | test acc:  0.9280\n",
      "Iter   180 | train loss  0.17 | test acc:  0.9257\n",
      "Iter   200 | train loss  0.27 | test acc:  0.9363\n",
      "Iter   220 | train loss  0.28 | test acc:  0.9343\n",
      "Iter   240 | train loss  0.20 | test acc:  0.9346\n",
      "Iter   260 | train loss  0.17 | test acc:  0.9418\n",
      "Iter   280 | train loss  0.17 | test acc:  0.9365\n",
      "Iter   300 | train loss  0.25 | test acc:  0.9358\n",
      "Iter   320 | train loss  0.21 | test acc:  0.9359\n",
      "Iter   340 | train loss  0.29 | test acc:  0.9504\n",
      "Iter   360 | train loss  0.26 | test acc:  0.9415\n",
      "Iter   380 | train loss  0.29 | test acc:  0.9390\n",
      "Iter   400 | train loss  0.25 | test acc:  0.9436\n",
      "Iter   420 | train loss  0.21 | test acc:  0.9476\n",
      "Iter   440 | train loss  0.13 | test acc:  0.9537\n",
      "Iter   460 | train loss  0.09 | test acc:  0.9442\n",
      "Iter   480 | train loss  0.27 | test acc:  0.9465\n",
      "Iter   500 | train loss  0.28 | test acc:  0.9539\n",
      "Iter   520 | train loss  0.17 | test acc:  0.9456\n",
      "Iter   540 | train loss  0.27 | test acc:  0.9483\n",
      "Iter   560 | train loss  0.17 | test acc:  0.9430\n",
      "Iter   580 | train loss  0.22 | test acc:  0.9412\n",
      "Iter   600 | train loss  0.12 | test acc:  0.9379\n",
      "Iter   620 | train loss  0.29 | test acc:  0.9377\n",
      "Iter   640 | train loss  0.44 | test acc:  0.9289\n",
      "Iter   660 | train loss  0.30 | test acc:  0.9545\n",
      "Iter   680 | train loss  0.15 | test acc:  0.9395\n",
      "Iter   700 | train loss  0.17 | test acc:  0.9540\n",
      "Iter   720 | train loss  0.21 | test acc:  0.9457\n",
      "Iter   740 | train loss  0.22 | test acc:  0.9374\n",
      "Iter   760 | train loss  0.37 | test acc:  0.9540\n",
      "Iter   780 | train loss  0.20 | test acc:  0.9534\n",
      "Iter   800 | train loss  0.14 | test acc:  0.9506\n",
      "Iter   820 | train loss  0.14 | test acc:  0.9501\n",
      "Iter   840 | train loss  0.06 | test acc:  0.9513\n",
      "Iter   860 | train loss  0.12 | test acc:  0.9519\n",
      "Iter   880 | train loss  0.17 | test acc:  0.9495\n",
      "Iter   900 | train loss  0.23 | test acc:  0.9476\n",
      "Iter   920 | train loss  0.18 | test acc:  0.9580\n",
      "Iter   940 | train loss  0.07 | test acc:  0.9540\n",
      "Iter   960 | train loss  0.12 | test acc:  0.9520\n",
      "Iter   980 | train loss  0.06 | test acc:  0.9590\n",
      "Iter  1000 | train loss  0.11 | test acc:  0.9531\n",
      "Iter  1020 | train loss  0.16 | test acc:  0.9592\n",
      "Iter  1040 | train loss  0.16 | test acc:  0.9617\n",
      "Iter  1060 | train loss  0.14 | test acc:  0.9628\n",
      "Iter  1080 | train loss  0.29 | test acc:  0.9482\n",
      "Iter  1100 | train loss  0.20 | test acc:  0.9557\n",
      "Iter  1120 | train loss  0.08 | test acc:  0.9571\n",
      "Iter  1140 | train loss  0.17 | test acc:  0.9573\n",
      "Iter  1160 | train loss  0.14 | test acc:  0.9589\n",
      "Iter  1180 | train loss  0.09 | test acc:  0.9520\n",
      "Iter  1200 | train loss  0.09 | test acc:  0.9621\n",
      "Iter  1220 | train loss  0.25 | test acc:  0.9586\n",
      "Iter  1240 | train loss  0.12 | test acc:  0.9535\n",
      "Iter  1260 | train loss  0.14 | test acc:  0.9505\n",
      "Iter  1280 | train loss  0.19 | test acc:  0.9547\n",
      "Iter  1300 | train loss  0.32 | test acc:  0.9517\n",
      "Iter  1320 | train loss  0.12 | test acc:  0.9573\n",
      "Iter  1340 | train loss  0.24 | test acc:  0.9569\n",
      "Iter  1360 | train loss  0.12 | test acc:  0.9545\n",
      "Iter  1380 | train loss  0.16 | test acc:  0.9559\n",
      "Iter  1400 | train loss  0.15 | test acc:  0.9449\n",
      "Iter  1420 | train loss  0.07 | test acc:  0.9541\n",
      "Iter  1440 | train loss  0.16 | test acc:  0.9572\n",
      "Iter  1460 | train loss  0.10 | test acc:  0.9561\n",
      "Iter  1480 | train loss  0.05 | test acc:  0.9540\n",
      "Iter  1500 | train loss  0.12 | test acc:  0.9529\n",
      "Iter  1520 | train loss  0.21 | test acc:  0.9510\n",
      "Iter  1540 | train loss  0.12 | test acc:  0.9585\n",
      "Iter  1560 | train loss  0.28 | test acc:  0.9611\n",
      "Iter  1580 | train loss  0.10 | test acc:  0.9584\n",
      "Iter  1600 | train loss  0.08 | test acc:  0.9523\n",
      "Iter  1620 | train loss  0.28 | test acc:  0.9535\n",
      "Iter  1640 | train loss  0.12 | test acc:  0.9570\n",
      "Iter  1660 | train loss  0.15 | test acc:  0.9576\n",
      "Iter  1680 | train loss  0.10 | test acc:  0.9608\n",
      "Iter  1700 | train loss  0.26 | test acc:  0.9642\n",
      "Iter  1720 | train loss  0.06 | test acc:  0.9574\n",
      "Iter  1740 | train loss  0.09 | test acc:  0.9515\n",
      "Iter  1760 | train loss  0.10 | test acc:  0.9616\n",
      "Iter  1780 | train loss  0.20 | test acc:  0.9522\n",
      "Iter  1800 | train loss  0.19 | test acc:  0.9581\n",
      "Iter  1820 | train loss  0.18 | test acc:  0.9618\n",
      "Iter  1840 | train loss  0.13 | test acc:  0.9615\n",
      "Iter  1860 | train loss  0.06 | test acc:  0.9643\n",
      "Iter  1880 | train loss  0.21 | test acc:  0.9581\n",
      "Iter  1900 | train loss  0.17 | test acc:  0.9575\n",
      "Iter  1920 | train loss  0.24 | test acc:  0.9589\n",
      "Iter  1940 | train loss  0.06 | test acc:  0.9433\n",
      "Iter  1960 | train loss  0.11 | test acc:  0.9495\n",
      "Iter  1980 | train loss  0.10 | test acc:  0.9562\n",
      "Iter  2000 | train loss  0.22 | test acc:  0.9515\n",
      "Iter  2020 | train loss  0.27 | test acc:  0.9585\n",
      "Iter  2040 | train loss  0.12 | test acc:  0.9606\n",
      "Iter  2060 | train loss  0.17 | test acc:  0.9611\n",
      "Iter  2080 | train loss  0.16 | test acc:  0.9624\n",
      "Iter  2100 | train loss  0.11 | test acc:  0.9587\n",
      "Iter  2120 | train loss  0.15 | test acc:  0.9588\n",
      "Iter  2140 | train loss  0.24 | test acc:  0.9590\n",
      "Iter  2160 | train loss  0.18 | test acc:  0.9470\n",
      "Iter  2180 | train loss  0.13 | test acc:  0.9571\n",
      "Iter  2200 | train loss  0.16 | test acc:  0.9557\n",
      "Iter  2220 | train loss  0.30 | test acc:  0.9556\n",
      "Iter  2240 | train loss  0.04 | test acc:  0.9621\n",
      "Iter  2260 | train loss  0.21 | test acc:  0.9573\n",
      "Iter  2280 | train loss  0.06 | test acc:  0.9616\n",
      "Iter  2300 | train loss  0.16 | test acc:  0.9535\n",
      "Iter  2320 | train loss  0.19 | test acc:  0.9578\n",
      "Iter  2340 | train loss  0.08 | test acc:  0.9553\n",
      "Iter  2360 | train loss  0.09 | test acc:  0.9603\n",
      "Iter  2380 | train loss  0.18 | test acc:  0.9367\n",
      "Iter  2400 | train loss  0.15 | test acc:  0.9602\n",
      "Iter  2420 | train loss  0.08 | test acc:  0.9518\n",
      "Iter  2440 | train loss  0.10 | test acc:  0.9578\n",
      "Iter  2460 | train loss  0.16 | test acc:  0.9631\n",
      "Iter  2480 | train loss  0.05 | test acc:  0.9582\n",
      "Iter  2500 | train loss  0.16 | test acc:  0.9600\n",
      "Iter  2520 | train loss  0.30 | test acc:  0.9558\n",
      "Iter  2540 | train loss  0.06 | test acc:  0.9573\n",
      "Iter  2560 | train loss  0.06 | test acc:  0.9578\n",
      "Iter  2580 | train loss  0.08 | test acc:  0.9594\n",
      "Iter  2600 | train loss  0.21 | test acc:  0.9566\n",
      "Iter  2620 | train loss  0.57 | test acc:  0.9608\n",
      "Iter  2640 | train loss  0.03 | test acc:  0.9591\n",
      "Iter  2660 | train loss  0.12 | test acc:  0.9618\n",
      "Iter  2680 | train loss  0.18 | test acc:  0.9620\n",
      "Iter  2700 | train loss  0.27 | test acc:  0.9607\n",
      "Iter  2720 | train loss  0.22 | test acc:  0.9625\n",
      "Iter  2740 | train loss  0.02 | test acc:  0.9608\n",
      "Iter  2760 | train loss  0.09 | test acc:  0.9584\n",
      "Iter  2780 | train loss  0.03 | test acc:  0.9592\n",
      "Iter  2800 | train loss  0.12 | test acc:  0.9676\n",
      "Iter  2820 | train loss  0.10 | test acc:  0.9651\n",
      "Iter  2840 | train loss  0.11 | test acc:  0.9616\n",
      "Iter  2860 | train loss  0.16 | test acc:  0.9614\n",
      "Iter  2880 | train loss  0.05 | test acc:  0.9613\n",
      "Iter  2900 | train loss  0.10 | test acc:  0.9654\n",
      "Iter  2920 | train loss  0.14 | test acc:  0.9611\n",
      "Iter  2940 | train loss  0.07 | test acc:  0.9648\n",
      "Iter  2960 | train loss  0.09 | test acc:  0.9585\n",
      "Iter  2980 | train loss  0.15 | test acc:  0.9637\n",
      "Iter  3000 | train loss  0.11 | test acc:  0.9610\n",
      "Iter  3020 | train loss  0.03 | test acc:  0.9660\n",
      "Iter  3040 | train loss  0.11 | test acc:  0.9636\n",
      "Iter  3060 | train loss  0.22 | test acc:  0.9573\n",
      "Iter  3080 | train loss  0.05 | test acc:  0.9650\n",
      "Iter  3100 | train loss  0.08 | test acc:  0.9660\n",
      "Iter  3120 | train loss  0.07 | test acc:  0.9609\n",
      "Iter  3140 | train loss  0.10 | test acc:  0.9664\n",
      "Iter  3160 | train loss  0.16 | test acc:  0.9679\n",
      "Iter  3180 | train loss  0.13 | test acc:  0.9669\n",
      "Iter  3200 | train loss  0.23 | test acc:  0.9648\n",
      "Iter  3220 | train loss  0.12 | test acc:  0.9619\n",
      "Iter  3240 | train loss  0.19 | test acc:  0.9658\n",
      "Iter  3260 | train loss  0.12 | test acc:  0.9649\n",
      "Iter  3280 | train loss  0.06 | test acc:  0.9627\n",
      "Iter  3300 | train loss  0.10 | test acc:  0.9630\n",
      "Iter  3320 | train loss  0.07 | test acc:  0.9629\n",
      "Iter  3340 | train loss  0.03 | test acc:  0.9658\n",
      "Iter  3360 | train loss  0.08 | test acc:  0.9597\n",
      "Iter  3380 | train loss  0.08 | test acc:  0.9617\n",
      "Iter  3400 | train loss  0.03 | test acc:  0.9626\n",
      "Iter  3420 | train loss  0.08 | test acc:  0.9589\n",
      "Iter  3440 | train loss  0.07 | test acc:  0.9609\n",
      "Iter  3460 | train loss  0.18 | test acc:  0.9669\n",
      "Iter  3480 | train loss  0.09 | test acc:  0.9595\n",
      "Iter  3500 | train loss  0.11 | test acc:  0.9624\n",
      "Iter  3520 | train loss  0.07 | test acc:  0.9619\n",
      "Iter  3540 | train loss  0.37 | test acc:  0.9658\n",
      "Iter  3560 | train loss  0.07 | test acc:  0.9626\n",
      "Iter  3580 | train loss  0.02 | test acc:  0.9599\n",
      "Iter  3600 | train loss  0.03 | test acc:  0.9597\n",
      "Iter  3620 | train loss  0.08 | test acc:  0.9641\n",
      "Iter  3640 | train loss  0.08 | test acc:  0.9590\n",
      "Iter  3660 | train loss  0.09 | test acc:  0.9638\n",
      "Iter  3680 | train loss  0.31 | test acc:  0.9546\n",
      "Iter  3700 | train loss  0.11 | test acc:  0.9408\n",
      "Iter  3720 | train loss  0.17 | test acc:  0.9573\n",
      "Iter  3740 | train loss  0.12 | test acc:  0.9517\n",
      "Iter  3760 | train loss  0.11 | test acc:  0.9636\n",
      "Iter  3780 | train loss  0.14 | test acc:  0.9636\n",
      "Iter  3800 | train loss  0.09 | test acc:  0.9663\n",
      "Iter  3820 | train loss  0.10 | test acc:  0.9657\n",
      "Iter  3840 | train loss  0.10 | test acc:  0.9665\n",
      "Iter  3860 | train loss  0.13 | test acc:  0.9636\n",
      "Iter  3880 | train loss  0.14 | test acc:  0.9627\n",
      "Iter  3900 | train loss  0.18 | test acc:  0.9624\n",
      "Iter  3920 | train loss  0.19 | test acc:  0.9662\n",
      "Iter  3940 | train loss  0.05 | test acc:  0.9566\n",
      "Iter  3960 | train loss  0.12 | test acc:  0.9582\n",
      "Iter  3980 | train loss  0.39 | test acc:  0.9446\n",
      "Iter  4000 | train loss  0.10 | test acc:  0.9636\n",
      "Iter  4020 | train loss  0.17 | test acc:  0.9635\n",
      "Iter  4040 | train loss  0.16 | test acc:  0.9622\n",
      "Iter  4060 | train loss  0.15 | test acc:  0.9592\n",
      "Iter  4080 | train loss  0.21 | test acc:  0.9590\n",
      "Iter  4100 | train loss  0.10 | test acc:  0.9558\n",
      "Iter  4120 | train loss  0.08 | test acc:  0.9567\n",
      "Iter  4140 | train loss  0.21 | test acc:  0.9503\n",
      "Iter  4160 | train loss  0.18 | test acc:  0.9607\n",
      "Iter  4180 | train loss  0.29 | test acc:  0.9564\n",
      "Iter  4200 | train loss  0.15 | test acc:  0.9621\n",
      "Iter  4220 | train loss  0.11 | test acc:  0.9552\n",
      "Iter  4240 | train loss  0.24 | test acc:  0.9608\n",
      "Iter  4260 | train loss  0.15 | test acc:  0.9513\n",
      "Iter  4280 | train loss  0.17 | test acc:  0.9479\n",
      "Iter  4300 | train loss  0.35 | test acc:  0.9544\n",
      "Iter  4320 | train loss  0.07 | test acc:  0.9585\n",
      "Iter  4340 | train loss  0.32 | test acc:  0.9514\n",
      "Iter  4360 | train loss  0.21 | test acc:  0.9594\n",
      "Iter  4380 | train loss  0.06 | test acc:  0.9617\n",
      "Iter  4400 | train loss  0.07 | test acc:  0.9621\n",
      "Iter  4420 | train loss  0.11 | test acc:  0.9569\n",
      "Iter  4440 | train loss  0.09 | test acc:  0.9516\n",
      "Iter  4460 | train loss  0.14 | test acc:  0.9562\n",
      "Iter  4480 | train loss  0.28 | test acc:  0.9619\n",
      "Iter  4500 | train loss  0.13 | test acc:  0.9550\n",
      "Iter  4520 | train loss  0.05 | test acc:  0.9643\n",
      "Iter  4540 | train loss  0.04 | test acc:  0.9640\n",
      "Iter  4560 | train loss  0.04 | test acc:  0.9627\n",
      "Iter  4580 | train loss  0.04 | test acc:  0.9642\n",
      "Iter  4600 | train loss  0.02 | test acc:  0.9607\n",
      "Iter  4620 | train loss  0.05 | test acc:  0.9587\n",
      "Iter  4640 | train loss  0.30 | test acc:  0.9582\n",
      "Iter  4660 | train loss  0.08 | test acc:  0.9562\n",
      "Iter  4680 | train loss  0.17 | test acc:  0.9620\n",
      "Iter  4700 | train loss  0.13 | test acc:  0.9627\n",
      "Iter  4720 | train loss  0.04 | test acc:  0.9631\n",
      "Iter  4740 | train loss  0.07 | test acc:  0.9652\n",
      "Iter  4760 | train loss  0.16 | test acc:  0.9624\n",
      "Iter  4780 | train loss  0.06 | test acc:  0.9592\n",
      "Iter  4800 | train loss  0.18 | test acc:  0.9638\n",
      "Iter  4820 | train loss  0.18 | test acc:  0.9570\n",
      "Iter  4840 | train loss  0.05 | test acc:  0.9585\n",
      "Iter  4860 | train loss  0.22 | test acc:  0.9565\n",
      "Iter  4880 | train loss  0.11 | test acc:  0.9558\n",
      "Iter  4900 | train loss  0.03 | test acc:  0.9610\n",
      "Iter  4920 | train loss  0.12 | test acc:  0.9593\n",
      "Iter  4940 | train loss  0.03 | test acc:  0.9655\n",
      "Iter  4960 | train loss  0.12 | test acc:  0.9621\n",
      "Iter  4980 | train loss  0.14 | test acc:  0.9571\n",
      "Iter  5000 | train loss  0.30 | test acc:  0.9601\n",
      "Iter  5020 | train loss  0.13 | test acc:  0.9633\n",
      "Iter  5040 | train loss  0.05 | test acc:  0.9648\n",
      "Iter  5060 | train loss  0.05 | test acc:  0.9641\n",
      "Iter  5080 | train loss  0.11 | test acc:  0.9597\n",
      "Iter  5100 | train loss  0.09 | test acc:  0.9614\n",
      "Iter  5120 | train loss  0.12 | test acc:  0.9625\n",
      "Iter  5140 | train loss  0.05 | test acc:  0.9613\n",
      "Iter  5160 | train loss  0.19 | test acc:  0.9645\n",
      "Iter  5180 | train loss  0.19 | test acc:  0.9574\n",
      "Iter  5200 | train loss  0.26 | test acc:  0.9662\n",
      "Iter  5220 | train loss  0.14 | test acc:  0.9622\n",
      "Iter  5240 | train loss  0.01 | test acc:  0.9625\n",
      "Iter  5260 | train loss  0.12 | test acc:  0.9651\n",
      "Iter  5280 | train loss  0.13 | test acc:  0.9620\n",
      "Iter  5300 | train loss  0.04 | test acc:  0.9551\n",
      "Iter  5320 | train loss  0.15 | test acc:  0.9635\n",
      "Iter  5340 | train loss  0.11 | test acc:  0.9578\n",
      "Iter  5360 | train loss  0.11 | test acc:  0.9625\n",
      "Iter  5380 | train loss  0.07 | test acc:  0.9586\n",
      "Iter  5400 | train loss  0.09 | test acc:  0.9578\n",
      "Iter  5420 | train loss  0.06 | test acc:  0.9608\n",
      "Iter  5440 | train loss  0.19 | test acc:  0.9659\n",
      "Iter  5460 | train loss  0.15 | test acc:  0.9619\n",
      "Iter  5480 | train loss  0.12 | test acc:  0.9646\n",
      "Iter  5500 | train loss  0.08 | test acc:  0.9620\n",
      "Iter  5520 | train loss  0.11 | test acc:  0.9643\n",
      "Iter  5540 | train loss  0.11 | test acc:  0.9611\n",
      "Iter  5560 | train loss  0.04 | test acc:  0.9645\n",
      "Iter  5580 | train loss  0.04 | test acc:  0.9607\n",
      "Iter  5600 | train loss  0.10 | test acc:  0.9684\n",
      "Iter  5620 | train loss  0.18 | test acc:  0.9647\n",
      "Iter  5640 | train loss  0.08 | test acc:  0.9661\n",
      "Iter  5660 | train loss  0.16 | test acc:  0.9680\n",
      "Iter  5680 | train loss  0.03 | test acc:  0.9633\n",
      "Iter  5700 | train loss  0.07 | test acc:  0.9609\n",
      "Iter  5720 | train loss  0.07 | test acc:  0.9566\n",
      "Iter  5740 | train loss  0.12 | test acc:  0.9630\n",
      "Iter  5760 | train loss  0.23 | test acc:  0.9431\n",
      "Iter  5780 | train loss  0.06 | test acc:  0.9559\n",
      "Iter  5800 | train loss  0.02 | test acc:  0.9541\n",
      "Iter  5820 | train loss  0.05 | test acc:  0.9615\n",
      "Iter  5840 | train loss  0.04 | test acc:  0.9659\n",
      "Iter  5860 | train loss  0.09 | test acc:  0.9651\n"
     ]
    }
   ],
   "source": [
    "no_aug_model = MLP(num_layers=5, input_shape=(28,28))\n",
    "no_aug_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(list(no_aug_model.parameters()), lr=lr)\n",
    "\n",
    "counter = 0\n",
    "for i in range(15):\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logit = no_aug_model(x)\n",
    "        train_loss = criterion(logit, y)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if counter % 20 == 0 and counter > 0:\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for x, y in test_loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    logit = no_aug_model(x)\n",
    "                    pred = torch.argmax(logit, dim=1).float()\n",
    "                    correct += (y == pred).float().sum().item()\n",
    "            acc = float(correct) / len(test_loader.dataset)\n",
    "            print(f\"Iter {counter:5d} | train loss {train_loss.item():5.2f} | test acc: {acc: 2.4f}\")\n",
    "    \n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}