{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ecfa63b96582461c8bb09ef7882650c11fd38af380cc5e83709fdd5945a664c6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Equibirium Model\n",
    "\n",
    "This class of models follows the formula\n",
    "\n",
    "$$\\min_{\\lambda=(\\gamma, \\theta)} = \\sum_{i=1}^n E_i(w_i(\\gamma), \\theta)$$\n",
    "$$w_i(\\gamma) = \\phi_i(w_i(\\gamma),\\gamma), \\quad i = 1\\dots n$$\n",
    "\n",
    "The experiment in the paper studies\n",
    "$$\\phi_i(w_i, \\gamma)=\\tanh(Aw_i + B x_i + c)$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd"
   ]
  },
  {
   "source": [
    "The following implementation or reparameterization of matrix $A$ allows its spectral norm strictly less than 1, $||A||<1$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HouseHolderMatrix(nn.Module):\n",
    "    \"\"\"Construct A in Householder transformation to make the contraction for the dynamic\"\"\"\n",
    "\n",
    "    def __init__(self, n_dims, rank=3):\n",
    "        super().__init__()\n",
    "        self.n_dims = n_dims\n",
    "        self.vectors = nn.ParameterList()\n",
    "        for _ in range(rank):\n",
    "            self.vectors.append(nn.Parameter(torch.randn(n_dims, 1)))\n",
    "        self.register_buffer(\"eye\", torch.eye(n_dims))\n",
    "    \n",
    "    def forward(self):\n",
    "        householder_matrices = [self.householder(v) for v in self.vectors]\n",
    "        if len(self.vectors) == 1:\n",
    "            return householder_matrices[0]\n",
    "        ret = householder_matrices[0]\n",
    "        for matrix in householder_matrices[1:]:\n",
    "            ret = ret @ matrix\n",
    "        return ret\n",
    "\n",
    "    \n",
    "    def householder(self, v):\n",
    "        return self.eye - 2.* v @ v.t() / torch.norm(v) ** 2\n",
    "\n",
    "\n",
    "class MatrixA(nn.Module):\n",
    "\n",
    "    def __init__(self, n_dims, rank=3):\n",
    "        super().__init__()\n",
    "        self.householder = HouseHolderMatrix(n_dims, rank)\n",
    "        self.diag = nn.Parameter(torch.randn(n_dims, ))\n",
    "    \n",
    "    def forward(self, epsilon=0.8):\n",
    "        D = torch.diag(torch.sigmoid(self.diag)* (1. - epsilon))\n",
    "        P = self.householder()\n",
    "        return (P @ D) @ P"
   ]
  },
  {
   "source": [
    "### Dynamic model in lower level\n",
    "In fact, this is just a fixed-point iteration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_data, input_dim, T=10):\n",
    "        super().__init__()\n",
    "        self.n_data = n_data\n",
    "        self.input_dim = input_dim\n",
    "        self.T = T\n",
    "\n",
    "        self.w = nn.Parameter(torch.zeros(n_data, input_dim))\n",
    "        \n",
    "    \n",
    "    def forward(self, x, dynamic_func):\n",
    "        \n",
    "        w = self.w\n",
    "        for _ in range(self.T):\n",
    "            w = dynamic_func(w, x)\n",
    "\n",
    "        return w\n",
    "\n",
    "class HyperModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_data, input_dim, hidden_dim, n_classes=10):\n",
    "\n",
    "        super().__init__()\n",
    "        # hyperparamters\n",
    "        self.A = MatrixA(hidden_dim, rank=3)\n",
    "        self.B = nn.Linear(input_dim, hidden_dim) \n",
    "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "        #parameters\n",
    "        self.dynamic_model = DynamicModel(n_data=n_data, input_dim=hidden_dim)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def dynamic_func(self, w, x):\n",
    "        return torch.tanh(w @ self.A() + self.B(x))\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        w = self.dynamic_model.forward(x, self.dynamic_func)\n",
    "        if not train:\n",
    "            logit = self.classifier(w)\n",
    "        else:\n",
    "            logit = None\n",
    "        return w, logit\n",
    "    \n",
    "    def validation_loss(self, x_val, y_val):\n",
    "        _, logit = self.forward(x_val, train=False)\n",
    "        loss = self.criterion(logit, y_val)\n",
    "        return loss\n",
    "    \n",
    "    def train_fixed_point_iteration(self, x_train, y_train):\n",
    "        w, _ = self.forward(x_train)\n",
    "        return w\n",
    "\n",
    "    @property\n",
    "    def hyper_parameters(self):\n",
    "        return list(self.A.parameters()) + list(self.B.parameters()) + list(self.classifier.parameters())\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return list(self.dynamic_model.parameters())"
   ]
  },
  {
   "source": [
    "### Prepare data\n",
    "Pick 5000 data point for train and validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "normalize = transforms.Compose([transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_data = datasets.MNIST(root=\".\", train=True)\n",
    "\n",
    "num_train = num_val = 5000\n",
    "x_train, y_train = train_data.data[:num_train], train_data.targets[:num_train]\n",
    "x_val, y_val = train_data.data[-num_val:], train_data.targets[-num_val:]\n",
    "def transform_data(x):\n",
    "    return torch.reshape(x / 255., (-1, 28*28))\n",
    "x_train, x_val = transform_data(x_train), transform_data(x_val)"
   ]
  },
  {
   "source": [
    "### Create models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim=200\n",
    "model = HyperModel(n_data=num_train, input_dim=28*28, hidden_dim=hidden_dim)\n",
    "hyper_optimizer = torch.optim.SGD(model.hyper_parameters, lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w, logit = model(x_val, train=False)\n",
    "        loss = model.criterion(logit, y_val)\n",
    "        pred = torch.argmax(logit, dim=1).float()\n",
    "        correct = (y_val == pred).float().sum()\n",
    "    return loss.item(), correct.item()/x_val.shape[0]\n",
    "    "
   ]
  },
  {
   "source": [
    "## Optimization\n",
    "The ```BaseHyperOpt``` cannot use here. The following is a slight variant implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iter 0 \t Val loss 2.302 \t Acc: 0.0902\n",
      "Iter 20 \t Val loss 2.194 \t Acc: 0.3302\n",
      "Iter 40 \t Val loss 2.092 \t Acc: 0.5572\n",
      "Iter 60 \t Val loss 1.992 \t Acc: 0.6836\n",
      "Iter 80 \t Val loss 1.893 \t Acc: 0.7336\n",
      "Iter 100 \t Val loss 1.795 \t Acc: 0.7614\n",
      "Iter 120 \t Val loss 1.698 \t Acc: 0.7782\n",
      "Iter 140 \t Val loss 1.604 \t Acc: 0.7916\n",
      "Iter 160 \t Val loss 1.513 \t Acc: 0.8020\n",
      "Iter 180 \t Val loss 1.426 \t Acc: 0.8088\n",
      "Iter 200 \t Val loss 1.344 \t Acc: 0.8158\n",
      "Iter 220 \t Val loss 1.268 \t Acc: 0.8206\n",
      "Iter 240 \t Val loss 1.198 \t Acc: 0.8262\n",
      "Iter 260 \t Val loss 1.133 \t Acc: 0.8298\n",
      "Iter 280 \t Val loss 1.075 \t Acc: 0.8358\n",
      "Iter 300 \t Val loss 1.022 \t Acc: 0.8404\n",
      "Iter 320 \t Val loss 0.974 \t Acc: 0.8448\n",
      "Iter 340 \t Val loss 0.930 \t Acc: 0.8472\n",
      "Iter 360 \t Val loss 0.890 \t Acc: 0.8506\n",
      "Iter 380 \t Val loss 0.854 \t Acc: 0.8548\n",
      "Iter 400 \t Val loss 0.821 \t Acc: 0.8580\n",
      "Iter 420 \t Val loss 0.791 \t Acc: 0.8616\n",
      "Iter 440 \t Val loss 0.764 \t Acc: 0.8640\n",
      "Iter 460 \t Val loss 0.739 \t Acc: 0.8680\n",
      "Iter 480 \t Val loss 0.716 \t Acc: 0.8718\n"
     ]
    }
   ],
   "source": [
    "n_iter = 500\n",
    "K = 20 \n",
    "for iter in range(n_iter):\n",
    "    # fixed point iteration\n",
    "    w = model.train_fixed_point_iteration(x_train, y_train)\n",
    "\n",
    "    # hypergradient step\n",
    "    val_loss = model.validation_loss(x_val, y_val)\n",
    "    dval_dparam = autograd.grad(\n",
    "        val_loss, \n",
    "        model.parameters,\n",
    "        retain_graph=True)\n",
    "    \n",
    "    def mvp(v):\n",
    "        return autograd.grad(\n",
    "            w,\n",
    "            model.parameters,\n",
    "            grad_outputs=v,\n",
    "            retain_graph=True\n",
    "        )\n",
    "    \n",
    "    v = dval_dparam\n",
    "    for k in range(K):\n",
    "        output = mvp(v)\n",
    "        v = [o_ + e_ for o_, e_ in zip(output, dval_dparam)]\n",
    "    \n",
    "    indirect = autograd.grad(\n",
    "        w,\n",
    "        model.hyper_parameters,\n",
    "        grad_outputs=v,\n",
    "        allow_unused=True\n",
    "    )\n",
    "\n",
    "    direct = autograd.grad(\n",
    "        val_loss,\n",
    "        model.hyper_parameters,\n",
    "        allow_unused=True\n",
    "    )\n",
    "\n",
    "    total_grad = []\n",
    "    for d, i in zip(direct, indirect):\n",
    "        if d is None and i is None:\n",
    "            raise RuntimeError(\"Both of them should not be None\")\n",
    "        elif d is None and i is not None:\n",
    "            total_grad.append(i)\n",
    "        elif i is None and d is not None:\n",
    "            total_grad.append(d)\n",
    "        else:\n",
    "            total_grad.append(d+i)\n",
    "        \n",
    "    hyper_optimizer.zero_grad()\n",
    "    for p, g in zip(model.hyper_parameters, total_grad):\n",
    "        p.grad = g\n",
    "    hyper_optimizer.step()\n",
    "\n",
    "    if iter % 20 == 0:\n",
    "        loss, acc = evaluate()\n",
    "        print(f\"Iter {iter} \\t Val loss {loss:.3f} \\t Acc: {acc:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}