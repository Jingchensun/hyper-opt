"""
MNISTæ•°æ®é€‰æ‹©ç¤ºä¾‹ï¼š
æ ¹æ®æ•°å­—7çš„éªŒè¯æŸå¤±ï¼Œä»0-9çš„æ‰€æœ‰è®­ç»ƒæ ·æœ¬ä¸­é€‰æ‹©æœ€ç›¸å…³çš„æ ·æœ¬
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset, Dataset
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
import argparse
import time

from model import (DataSelectionHyperOptModel, LabelBasedDataSelectionModel, 
                   FeatureMlpDataSelectionModel, CnnFeatureDataSelectionModel)
from hyper_opt import NeumannHyperOptimizer, FixedPointHyperOptimizer
from network import SimpleModel


def get_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='MNISTæ•°æ®é€‰æ‹©è¶…å‚æ•°ä¼˜åŒ–')
    
    # æ¨¡å‹é€‰æ‹©å‚æ•°
    parser.add_argument('--selector', type=str, default='label_based',
                        choices=['original', 'label_based', 'feature_mlp', 'cnn_feature'],
                        help='é€‰æ‹©æ•°æ®é€‰æ‹©æ¨¡å‹ç±»å‹')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=2,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=256,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--fast_mode', action='store_true',
                        help='ä½¿ç”¨å¿«é€Ÿè®­ç»ƒæ¨¡å¼')
    parser.add_argument('--val_subset_size', type=int, default=2000,
                        help='å¿«é€Ÿæ¨¡å¼ä¸‹çš„éªŒè¯é›†å­é›†å¤§å°')
    parser.add_argument('--hyper_opt_freq', type=int, default=10,
                        help='å¿«é€Ÿæ¨¡å¼ä¸‹çš„è¶…å‚æ•°ä¼˜åŒ–é¢‘ç‡')
    
    # åˆ†æå‚æ•°
    parser.add_argument('--top_k', type=int, default=100,
                        help='åˆ†ætop-ké‡è¦æ ·æœ¬')
    parser.add_argument('--vis_samples', type=int, default=20,
                        help='å¯è§†åŒ–æ ·æœ¬æ•°é‡')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--target_digit', type=int, default=7,
                        help='ç›®æ ‡æ•°å­—')
    parser.add_argument('--device', type=str, default='auto',
                        help='è®¾å¤‡é€‰æ‹© (auto/cpu/cuda)')
    
    return parser.parse_args()


def print_model_info(model_type):
    """æ‰“å°æ¨¡å‹ç±»å‹ä¿¡æ¯"""
    model_info = {
        'original': {
            'name': 'åŸå§‹æ¯æ ·æœ¬æƒé‡æ¨¡å‹',
            'params': '60,000ä¸ª',
            'pros': 'æœ€ç²¾ç»†æ§åˆ¶',
            'cons': 'å‚æ•°è¿‡å¤šï¼Œéš¾ä¼˜åŒ–ï¼Œæ˜“è¿‡æ‹Ÿåˆ',
            'speed': 'â­',
            'recommend': 'âŒ'
        },
        'label_based': {
            'name': 'åŸºäºæ ‡ç­¾çš„æ¨¡å‹',
            'params': '10ä¸ª',
            'pros': 'æœ€ç®€å•ï¼Œæœ€å¿«ï¼Œæ˜“è§£é‡Š',
            'cons': 'ç²’åº¦è¾ƒç²—',
            'speed': 'â­â­â­â­â­',
            'recommend': 'ğŸ¥‡ æ¨è'
        },
        'feature_mlp': {
            'name': 'åŸºäºç‰¹å¾MLPçš„æ¨¡å‹',
            'params': '~500ä¸ª',
            'pros': 'ç‰¹å¾åŠ¨æ€åˆ†é…ï¼Œè¾ƒç²¾ç»†',
            'cons': 'éœ€æ‰‹å·¥ç‰¹å¾è®¾è®¡',
            'speed': 'â­â­â­â­',
            'recommend': 'ğŸ¥ˆ'
        },
        'cnn_feature': {
            'name': 'åŸºäºCNNç‰¹å¾çš„æ¨¡å‹',
            'params': '~2000ä¸ª',
            'pros': 'è‡ªåŠ¨å­¦ä¹ ç‰¹å¾ï¼Œæœ€çµæ´»',
            'cons': 'å‚æ•°æœ€å¤šï¼Œè®­ç»ƒæœ€æ…¢',
            'speed': 'â­â­â­',
            'recommend': 'ğŸ¥‰'
        }
    }
    
    info = model_info[model_type]
    print(f"ğŸ§  é€‰æ‹©çš„æ¨¡å‹: {info['name']}")
    print(f"   ğŸ“Š å‚æ•°æ•°é‡: {info['params']}")
    print(f"   âœ… ä¼˜ç‚¹: {info['pros']}")
    print(f"   âš ï¸  ç¼ºç‚¹: {info['cons']}")
    print(f"   ğŸš€ è®­ç»ƒé€Ÿåº¦: {info['speed']}")
    print(f"   ğŸ† æ¨èåº¦: {info['recommend']}")


def setup_device(device_arg):
    """è®¾ç½®è®¡ç®—è®¾å¤‡"""
    if device_arg == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(device_arg)
    
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    return device


def print_training_stats(selector, model_type):
    """æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in selector.model.parameters):,}")
    print(f"ğŸ¯ è¶…å‚æ•°æ•°é‡: {sum(p.numel() for p in selector.model.hyper_parameters):,}")
    print(f"âš™ï¸  è®­ç»ƒæ‰¹æ¬¡å¤§å°: {selector.batch_size}")
    print(f"ğŸ”„ æ¯ä¸ªepochæ‰¹æ¬¡æ•°: {len(selector.train_loader)}")
    
    # æ˜¾ç¤ºåˆå§‹æƒé‡ç»Ÿè®¡
    print_initial_weights(selector, model_type)


def print_initial_weights(selector, model_type):
    """æ‰“å°åˆå§‹æƒé‡ç»Ÿè®¡"""
    if model_type == 'original':
        initial_weights = selector.model.get_sample_weights().detach().cpu().numpy()
        print(f"\nğŸ“ˆ åˆå§‹æ ·æœ¬æƒé‡ç»Ÿè®¡:")
        print(f"   å¹³å‡æƒé‡: {initial_weights.mean():.6f}")
        print(f"   æƒé‡æ ‡å‡†å·®: {initial_weights.std():.6f}")
        print(f"   æƒé‡èŒƒå›´: [{initial_weights.min():.6f}, {initial_weights.max():.6f}]")
    elif model_type == 'label_based':
        initial_class_weights = selector.model.get_class_weights().detach().cpu().numpy()
        print(f"\nğŸ“ˆ åˆå§‹ç±»åˆ«æƒé‡ç»Ÿè®¡:")
        for i, weight in enumerate(initial_class_weights):
            print(f"   æ•°å­— {i}: {weight:.6f}")
        print(f"   å¹³å‡æƒé‡: {initial_class_weights.mean():.6f}")
        print(f"   æƒé‡èŒƒå›´: [{initial_class_weights.min():.6f}, {initial_class_weights.max():.6f}]")
    else:
        print(f"\nğŸ“ˆ åˆå§‹æƒé‡ç½‘ç»œå‚æ•°ç»Ÿè®¡:")
        total_params = sum(p.numel() for p in selector.model.hyper_parameters)
        print(f"   è¶…å‚æ•°æ€»æ•°: {total_params}")
        all_params = torch.cat([p.flatten() for p in selector.model.hyper_parameters])
        print(f"   å‚æ•°èŒƒå›´: [{all_params.min():.6f}, {all_params.max():.6f}]")
        print(f"   å‚æ•°å‡å€¼: {all_params.mean():.6f}")
        print(f"   å‚æ•°æ ‡å‡†å·®: {all_params.std():.6f}")


def train_model(selector, args):
    """è®­ç»ƒæ¨¡å‹"""
    print(f"\nâ° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # é€‰æ‹©è®­ç»ƒæ¨¡å¼
    if args.fast_mode:
        print(f"ğŸš€ ä½¿ç”¨å¿«é€Ÿè®­ç»ƒæ¨¡å¼")
        train_losses, val_losses = selector.train_fast(
            num_epochs=args.epochs,
            val_subset_size=args.val_subset_size,
            hyper_opt_freq=args.hyper_opt_freq
        )
    else:
        print(f"ğŸ¯ ä½¿ç”¨å®Œæ•´è®­ç»ƒæ¨¡å¼")
        train_losses, val_losses = selector.train(num_epochs=args.epochs)
    
    return train_losses, val_losses


def analyze_and_visualize(selector, args):
    """åˆ†ææ ·æœ¬é‡è¦æ€§å¹¶å¯è§†åŒ–"""
    print(f"\nğŸ“Š åˆ†ææ ·æœ¬é‡è¦æ€§...")
    top_indices, top_weights = selector.analyze_sample_importance(top_k=args.top_k)
    
    print(f"\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    # ä¸ºå¯è§†åŒ–å‡†å¤‡æƒé‡æ•°ç»„
    all_weights = prepare_weights_for_visualization(selector, args.selector)
    selector.visualize_results(top_indices, all_weights, num_samples=args.vis_samples)
    
    return all_weights


def prepare_weights_for_visualization(selector, model_type):
    """ä¸ºå¯è§†åŒ–å‡†å¤‡å®Œæ•´çš„æƒé‡æ•°ç»„"""
    if model_type == 'label_based':
        class_weights = selector.model.get_class_weights().detach().cpu().numpy()
        transform = transforms.Compose([transforms.ToTensor()])
        dataset = datasets.MNIST('data', train=True, transform=transform)
        all_weights = []
        for train_idx in selector.train_indices:
            _, label = dataset[train_idx]
            all_weights.append(class_weights[label])
        return np.array(all_weights)
    
    elif model_type == 'original':
        return selector.model.get_sample_weights().detach().cpu().numpy()
    
    else:  # feature_mlp æˆ– cnn_feature
        all_weights = []
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        dataset = datasets.MNIST('data', train=True, transform=transform)
        
        batch_size = 256
        for i in range(0, len(selector.train_indices), batch_size):
            batch_indices = selector.train_indices[i:i+batch_size]
            batch_data = []
            
            for idx in batch_indices:
                data, _ = dataset[idx]
                batch_data.append(data)
            
            if len(batch_data) > 0:
                batch_tensor = torch.stack(batch_data).to(selector.device)
                with torch.no_grad():
                    batch_weights = selector.model.get_sample_weights(batch_tensor)
                    all_weights.extend(batch_weights.detach().cpu().numpy())
        
        return np.array(all_weights)


def plot_results(train_losses, val_losses, all_weights):
    """ç»˜åˆ¶è®­ç»ƒç»“æœ"""
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±')
    plt.plot(val_losses, label='éªŒè¯æŸå¤± (æ•°å­—7)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('è®­ç»ƒè¿‡ç¨‹')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.hist(all_weights, bins=50, alpha=0.7)
    plt.xlabel('æ ·æœ¬æƒé‡')
    plt.ylabel('é¢‘æ•°')
    plt.title('æ ·æœ¬æƒé‡åˆ†å¸ƒ')
    
    plt.tight_layout()
    plt.show()


def print_performance_summary(start_time, train_losses, val_losses, selector):
    """æ‰“å°æ€§èƒ½æ€»ç»“"""
    end_time = time.time()
    training_time = end_time - start_time
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {training_time:.1f} ç§’ ({training_time/60:.1f} åˆ†é’Ÿ)")
    print(f"ğŸ“‰ æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}")
    print(f"ğŸ“Š æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.4f}")
    
    # æ€§èƒ½åˆ†ææ€»ç»“
    if hasattr(selector, 'timing_stats') and selector.timing_stats['total']:
        avg_batch_time = np.mean(selector.timing_stats['total'])
        avg_val_time = np.mean(selector.timing_stats['val'])
        avg_hyper_time = np.mean(selector.timing_stats['hyper'])
        
        print(f"\nğŸ“ˆ æ€§èƒ½åˆ†ææ€»ç»“:")
        print(f"   å¹³å‡æ¯batchæ—¶é—´: {avg_batch_time:.2f}s")
        print(f"   éªŒè¯æŸå¤±è®¡ç®—: {avg_val_time:.2f}s ({avg_val_time/avg_batch_time*100:.1f}%)")
        print(f"   è¶…å‚æ•°ä¼˜åŒ–: {avg_hyper_time:.2f}s ({avg_hyper_time/avg_batch_time*100:.1f}%)")


class IndexedDataset(Dataset):
    """å¸¦ç´¢å¼•çš„æ•°æ®é›†ï¼Œç”¨äºè·Ÿè¸ªæ ·æœ¬åœ¨åŸå§‹è®­ç»ƒé›†ä¸­çš„ä½ç½®"""
    def __init__(self, base_dataset, indices):
        self.base_dataset = base_dataset
        self.indices = indices
        
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        real_idx = self.indices[idx]
        data, target = self.base_dataset[real_idx]
        return data, target, idx  # è¿”å›åœ¨train_indicesä¸­çš„ä½ç½®


class MNISTDataSelector:
    def __init__(self, target_digits=[7], batch_size=64, device='cpu', model_type='label_based'):
        self.target_digits = target_digits
        self.batch_size = batch_size
        self.device = device
        self.model_type = model_type
        
        # åŠ è½½MNISTæ•°æ®
        self.train_loader, self.val_loader, self.train_indices = self._load_data()
        
        # åˆ›å»ºæ¨¡å‹
        self.network = SimpleModel().to(device)
        self.criterion = nn.CrossEntropyLoss(reduction='none')  # æ³¨æ„è¿™é‡Œéœ€è¦reduction='none'
        
        # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºæ•°æ®é€‰æ‹©æ¨¡å‹
        num_train_samples = len(self.train_indices)
        self.model = self._create_selection_model(num_train_samples).to(device)
        
        # åˆ›å»ºè¶…å‚æ•°ä¼˜åŒ–å™¨
        self.hyper_optimizer = NeumannHyperOptimizer(
            parameters=list(self.model.parameters),
            hyper_parameters=self.model.hyper_parameters,
            base_optimizer='SGD',
            default=dict(lr=0.01),
            use_gauss_newton=True,
            stochastic=True
        )
        # è®¾ç½®è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°ä»¥æé«˜é€Ÿåº¦
        self.hyper_optimizer.set_kwargs(inner_lr=0.1, K=5)  # é»˜è®¤K=20ï¼Œç°åœ¨è®¾ä¸º5
        
        # åˆ›å»ºæ¨¡å‹å‚æ•°ä¼˜åŒ–å™¨
        self.model_optimizer = torch.optim.SGD(self.model.parameters, lr=0.01)
        
    def _load_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†MNISTæ•°æ®"""
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        # åŠ è½½å®Œæ•´æ•°æ®é›†
        train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)
        test_dataset = datasets.MNIST('data', train=False, transform=transform)
        
        # åˆ†ç¦»éªŒè¯é›†ï¼ˆåªåŒ…å«ç›®æ ‡æ•°å­—7ï¼‰
        val_indices = []
        train_indices = []
        
        for idx, (data, target) in enumerate(train_dataset):
            # æ‰€æœ‰æ ·æœ¬éƒ½åŠ å…¥è®­ç»ƒé›†
            train_indices.append(idx)
            # åªæœ‰ç›®æ ‡æ•°å­—åŠ å…¥éªŒè¯é›†
            if target in self.target_digits:
                val_indices.append(idx)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_indexed_dataset = IndexedDataset(train_dataset, train_indices)
        val_subset = Subset(train_dataset, val_indices)
        
        train_loader = DataLoader(train_indexed_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=self.batch_size, shuffle=False)
        
        print(f"è®­ç»ƒæ ·æœ¬æ•°é‡: {len(train_indices)}")
        print(f"éªŒè¯æ ·æœ¬æ•°é‡ (æ•°å­—{self.target_digits}): {len(val_indices)}")
        
        return train_loader, val_loader, train_indices
    
    def _create_selection_model(self, num_train_samples):
        """æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºç›¸åº”çš„æ•°æ®é€‰æ‹©æ¨¡å‹"""
        if self.model_type == 'original':
            # åŸå§‹çš„æ¯æ ·æœ¬æƒé‡æ¨¡å‹
            return DataSelectionHyperOptModel(
                self.network, 
                self.criterion, 
                num_train_samples
            )
        elif self.model_type == 'label_based':
            # åŸºäºæ ‡ç­¾çš„ç®€åŒ–æ¨¡å‹ï¼ˆæ¨èï¼‰
            return LabelBasedDataSelectionModel(
                self.network, 
                self.criterion,
                num_classes=10
            )
        elif self.model_type == 'feature_mlp':
            # åŸºäºç‰¹å¾MLPçš„æ¨¡å‹
            return FeatureMlpDataSelectionModel(
                self.network, 
                self.criterion,
                feature_dim=16,
                hidden_dim=32
            )
        elif self.model_type == 'cnn_feature':
            # åŸºäºCNNç‰¹å¾çš„æ¨¡å‹
            return CnnFeatureDataSelectionModel(
                self.network, 
                self.criterion,
                hidden_dim=64
            )
        else:
            raise ValueError(f"Unsupported model_type: {self.model_type}. "
                           f"Choose from: 'original', 'label_based', 'feature_mlp', 'cnn_feature'")
    
    def train_step(self, train_data, train_targets, train_batch_indices):
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        import time
        
        step_start = time.time()
        
        # å®šä¹‰è®­ç»ƒæŸå¤±å‡½æ•°
        def train_loss_func():
            # é‡æ–°è®¡ç®—è®­ç»ƒæŸå¤±ï¼ˆç”¨äºéšæœºæ¨¡å¼ï¼‰
            train_loss, train_logit = self.model.train_loss(
                train_data, train_targets, train_batch_indices
            )
            return train_loss, train_logit
        
        # è®¡ç®—éªŒè¯æŸå¤± (è¿™é€šå¸¸æ˜¯æœ€æ…¢çš„éƒ¨åˆ†)
        val_start = time.time()
        val_loss = self._compute_validation_loss()
        val_time = time.time() - val_start
        
        # è¶…å‚æ•°ä¼˜åŒ–æ­¥éª¤
        hyper_start = time.time()
        self.hyper_optimizer.step(train_loss_func, val_loss)
        hyper_time = time.time() - hyper_start
        
        # æ¨¡å‹å‚æ•°ä¼˜åŒ–æ­¥éª¤
        model_start = time.time()
        self.model_optimizer.zero_grad()
        train_loss, _ = train_loss_func()
        train_loss.backward()
        self.model_optimizer.step()
        model_time = time.time() - model_start
        
        total_time = time.time() - step_start
        
        # å­˜å‚¨æ—¶é—´ç»Ÿè®¡ç”¨äºåˆ†æ
        if not hasattr(self, 'timing_stats'):
            self.timing_stats = {'val': [], 'hyper': [], 'model': [], 'total': []}
        
        self.timing_stats['val'].append(val_time)
        self.timing_stats['hyper'].append(hyper_time)
        self.timing_stats['model'].append(model_time)
        self.timing_stats['total'].append(total_time)
        
        return train_loss.item(), val_loss.item(), {
            'val_time': val_time,
            'hyper_time': hyper_time, 
            'model_time': model_time,
            'total_time': total_time
        }
    
    def _compute_validation_loss(self):
        """è®¡ç®—éªŒè¯æŸå¤±"""
        total_loss = 0
        total_samples = 0
        
        # ä¸ä½¿ç”¨no_grad()ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦è®¡ç®—éªŒè¯æŸå¤±çš„æ¢¯åº¦
        for val_data, val_targets in self.val_loader:
            val_data, val_targets = val_data.to(self.device), val_targets.to(self.device)
            val_loss = self.model.validation_loss(val_data, val_targets)
            total_loss += val_loss * val_data.size(0)  # ç›´æ¥ä½¿ç”¨tensorï¼Œä¸è½¬æ¢ä¸ºitem
            total_samples += val_data.size(0)
        
        return total_loss / total_samples
    
    def train(self, num_epochs=50):
        """è®­ç»ƒæ•°æ®é€‰æ‹©æ¨¡å‹"""
        train_losses = []
        val_losses = []
        
        print("å¼€å§‹è®­ç»ƒæ•°æ®é€‰æ‹©æ¨¡å‹...")
        print("=" * 80)
        
        for epoch in range(num_epochs):
            epoch_train_loss = 0
            epoch_val_loss = 0
            num_batches = 0
            
            # æ¯ä¸ªepochå¼€å§‹æ—¶æ˜¾ç¤ºæƒé‡ç»Ÿè®¡
            if epoch % 5 == 0:
                if self.model_type == 'original':
                    weights = self.model.get_sample_weights().detach().cpu().numpy()
                    print(f"\nEpoch {epoch} - æ ·æœ¬æƒé‡ç»Ÿè®¡:")
                    print(f"  æœ€å¤§æƒé‡: {weights.max():.6f}")
                    print(f"  æœ€å°æƒé‡: {weights.min():.6f}")
                    print(f"  å¹³å‡æƒé‡: {weights.mean():.6f}")
                    print(f"  æƒé‡æ ‡å‡†å·®: {weights.std():.6f}")
                    print(f"  é«˜æƒé‡æ ·æœ¬æ•° (>avg): {(weights > weights.mean()).sum()}")
                elif self.model_type == 'label_based':
                    class_weights = self.model.get_class_weights().detach().cpu().numpy()
                    print(f"\nEpoch {epoch} - ç±»åˆ«æƒé‡ç»Ÿè®¡:")
                    for i, weight in enumerate(class_weights):
                        print(f"  æ•°å­— {i}: {weight:.6f}")
                    print(f"  æœ€å¤§ç±»åˆ«æƒé‡: {class_weights.max():.6f}")
                    print(f"  æœ€å°ç±»åˆ«æƒé‡: {class_weights.min():.6f}")
                else:
                    print(f"\nEpoch {epoch} - æƒé‡ç½‘ç»œå‚æ•°ç»Ÿè®¡:")
                    total_params = sum(p.numel() for p in self.model.hyper_parameters)
                    print(f"  è¶…å‚æ•°æ•°é‡: {total_params}")
                    # æ˜¾ç¤ºæƒé‡ç½‘ç»œçš„å‚æ•°èŒƒå›´
                    all_params = torch.cat([p.flatten() for p in self.model.hyper_parameters])
                    print(f"  å‚æ•°èŒƒå›´: [{all_params.min():.6f}, {all_params.max():.6f}]")
            
            for batch_idx, (train_data, train_targets, train_batch_indices) in enumerate(self.train_loader):
                train_data, train_targets = train_data.to(self.device), train_targets.to(self.device)
                # train_batch_indices å·²ç»ä» IndexedDataset ä¸­è·å¾—
                
                # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
                train_loss, val_loss, timing = self.train_step(
                    train_data, train_targets, train_batch_indices
                )
                
                epoch_train_loss += train_loss
                epoch_val_loss += val_loss
                num_batches += 1
                
                # æ¯20ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
                if batch_idx % 20 == 0:
                    if self.model_type == 'original':
                        current_weights = self.model.get_sample_weights()[train_batch_indices].detach().cpu().numpy()
                        weight_info = f"å½“å‰batchæƒé‡: {current_weights.mean():.4f}Â±{current_weights.std():.4f}"
                    elif self.model_type == 'label_based':
                        batch_weights = self.model.get_sample_weights_by_labels(train_targets).detach().cpu().numpy()
                        weight_info = f"å½“å‰batchæƒé‡: {batch_weights.mean():.4f}Â±{batch_weights.std():.4f}"
                    else:
                        # feature_mlp or cnn_feature: å®æ—¶è®¡ç®—æƒé‡
                        with torch.no_grad():
                            batch_weights = self.model.get_sample_weights(train_data).detach().cpu().numpy()
                        weight_info = f"å½“å‰batchæƒé‡: {batch_weights.mean():.4f}Â±{batch_weights.std():.4f}"
                    
                    print(f"  Epoch {epoch:2d}, Batch {batch_idx:3d}/{len(self.train_loader):3d} | "
                          f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} | {weight_info}")
                    print(f"    â±ï¸  æ—¶é—´åˆ†æ: éªŒè¯æŸå¤± {timing['val_time']:.2f}s, "
                          f"è¶…å‚æ•°ä¼˜åŒ– {timing['hyper_time']:.2f}s, "
                          f"æ¨¡å‹ä¼˜åŒ– {timing['model_time']:.2f}s, "
                          f"æ€»è®¡ {timing['total_time']:.2f}s")
            
            avg_train_loss = epoch_train_loss / num_batches
            avg_val_loss = epoch_val_loss / num_batches
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            # æ¯ä¸ªepochç»“æŸæ—¶æ‰“å°æ€»ç»“
            print(f"\n>>> Epoch {epoch:2d} å®Œæˆ | "
                  f"å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, å¹³å‡éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
            
            # æ˜¾ç¤ºæŸå¤±å˜åŒ–è¶‹åŠ¿
            if epoch > 0:
                train_change = avg_train_loss - train_losses[-2]
                val_change = avg_val_loss - val_losses[-2]
                train_trend = "â†“" if train_change < 0 else "â†‘" if train_change > 0 else "="
                val_trend = "â†“" if val_change < 0 else "â†‘" if val_change > 0 else "="
                print(f"    æŸå¤±å˜åŒ–: è®­ç»ƒ {train_trend} {train_change:+.4f}, éªŒè¯ {val_trend} {val_change:+.4f}")
            
            # æ˜¾ç¤ºè¿™ä¸ªepochçš„å¹³å‡æ—¶é—´ç»Ÿè®¡
            if hasattr(self, 'timing_stats'):
                import numpy as np
                recent_stats = {}
                for key in self.timing_stats:
                    recent_stats[key] = np.mean(self.timing_stats[key][-num_batches:])
                
                print(f"    â±ï¸  å¹³å‡æ—¶é—´åˆ†æ: éªŒè¯æŸå¤± {recent_stats['val']:.2f}s ({recent_stats['val']/recent_stats['total']*100:.1f}%), "
                      f"è¶…å‚æ•°ä¼˜åŒ– {recent_stats['hyper']:.2f}s ({recent_stats['hyper']/recent_stats['total']*100:.1f}%), "
                      f"æ¨¡å‹ä¼˜åŒ– {recent_stats['model']:.2f}s ({recent_stats['model']/recent_stats['total']*100:.1f}%)")
                print(f"    ğŸ“Š æ¯batchå¹³å‡æ—¶é—´: {recent_stats['total']:.2f}s, é¢„è®¡å®Œæˆæ—¶é—´: {recent_stats['total'] * len(self.train_loader) * (num_epochs - epoch - 1) / 60:.1f} åˆ†é’Ÿ")
            
            print("-" * 80)
        
        return train_losses, val_losses
    
    def train_fast(self, num_epochs=10, val_subset_size=1000, hyper_opt_freq=5):
        """å¿«é€Ÿè®­ç»ƒç‰ˆæœ¬ - ä¼˜åŒ–æ€§èƒ½"""
        print("ğŸš€ ä½¿ç”¨å¿«é€Ÿè®­ç»ƒæ¨¡å¼ (æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬)")
        print(f"   - éªŒè¯é›†å­é›†å¤§å°: {val_subset_size}")
        print(f"   - è¶…å‚æ•°ä¼˜åŒ–é¢‘ç‡: æ¯ {hyper_opt_freq} ä¸ªbatch")
        
        # åˆ›å»ºéªŒè¯é›†å­é›†
        val_subset_indices = torch.randperm(len(self.val_loader.dataset))[:val_subset_size]
        
        train_losses = []
        val_losses = []
        
        print("=" * 80)
        
        for epoch in range(num_epochs):
            epoch_train_loss = 0
            epoch_val_loss = 0
            num_batches = 0
            
            # æ¯ä¸ªepochå¼€å§‹æ—¶æ˜¾ç¤ºæƒé‡ç»Ÿè®¡
            if epoch % 5 == 0:
                if self.model_type == 'original':
                    weights = self.model.get_sample_weights().detach().cpu().numpy()
                    print(f"\nEpoch {epoch} - æ ·æœ¬æƒé‡ç»Ÿè®¡:")
                    print(f"  æœ€å¤§æƒé‡: {weights.max():.6f}")
                    print(f"  æœ€å°æƒé‡: {weights.min():.6f}")
                    print(f"  å¹³å‡æƒé‡: {weights.mean():.6f}")
                    print(f"  æƒé‡æ ‡å‡†å·®: {weights.std():.6f}")
                    print(f"  é«˜æƒé‡æ ·æœ¬æ•° (>avg): {(weights > weights.mean()).sum()}")
                elif self.model_type == 'label_based':
                    class_weights = self.model.get_class_weights().detach().cpu().numpy()
                    print(f"\nEpoch {epoch} - ç±»åˆ«æƒé‡ç»Ÿè®¡:")
                    for i, weight in enumerate(class_weights):
                        print(f"  æ•°å­— {i}: {weight:.6f}")
                    print(f"  æœ€å¤§ç±»åˆ«æƒé‡: {class_weights.max():.6f}")
                    print(f"  æœ€å°ç±»åˆ«æƒé‡: {class_weights.min():.6f}")
                else:
                    print(f"\nEpoch {epoch} - æƒé‡ç½‘ç»œå‚æ•°ç»Ÿè®¡:")
                    total_params = sum(p.numel() for p in self.model.hyper_parameters)
                    print(f"  è¶…å‚æ•°æ•°é‡: {total_params}")
                    # æ˜¾ç¤ºæƒé‡ç½‘ç»œçš„å‚æ•°èŒƒå›´
                    all_params = torch.cat([p.flatten() for p in self.model.hyper_parameters])
                    print(f"  å‚æ•°èŒƒå›´: [{all_params.min():.6f}, {all_params.max():.6f}]")
            
            for batch_idx, (train_data, train_targets, train_batch_indices) in enumerate(self.train_loader):
                train_data, train_targets = train_data.to(self.device), train_targets.to(self.device)
                
                # åªåœ¨æŒ‡å®šé¢‘ç‡ä¸‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
                if batch_idx % hyper_opt_freq == 0:
                    train_loss, val_loss, timing = self.train_step_fast(
                        train_data, train_targets, train_batch_indices, val_subset_indices
                    )
                else:
                    # åªè¿›è¡Œæ¨¡å‹å‚æ•°ä¼˜åŒ–
                    train_loss, val_loss, timing = self.train_step_model_only(
                        train_data, train_targets, train_batch_indices
                    )
                
                epoch_train_loss += train_loss
                epoch_val_loss += val_loss
                num_batches += 1
                
                # æ¯50ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
                if batch_idx % 50 == 0:
                    if self.model_type == 'original':
                        current_weights = self.model.get_sample_weights()[train_batch_indices].detach().cpu().numpy()
                        weight_info = f"å½“å‰batchæƒé‡: {current_weights.mean():.4f}Â±{current_weights.std():.4f}"
                    elif self.model_type == 'label_based':
                        batch_weights = self.model.get_sample_weights_by_labels(train_targets).detach().cpu().numpy()
                        weight_info = f"å½“å‰batchæƒé‡: {batch_weights.mean():.4f}Â±{batch_weights.std():.4f}"
                    else:
                        # feature_mlp or cnn_feature: å®æ—¶è®¡ç®—æƒé‡
                        with torch.no_grad():
                            batch_weights = self.model.get_sample_weights(train_data).detach().cpu().numpy()
                        weight_info = f"å½“å‰batchæƒé‡: {batch_weights.mean():.4f}Â±{batch_weights.std():.4f}"
                    
                    print(f"  Epoch {epoch:2d}, Batch {batch_idx:3d}/{len(self.train_loader):3d} | "
                          f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} | {weight_info}")
                    if timing:
                        print(f"    â±ï¸  æ—¶é—´: {timing['total_time']:.2f}s")
            
            avg_train_loss = epoch_train_loss / num_batches
            avg_val_loss = epoch_val_loss / num_batches
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            # æ¯ä¸ªepochç»“æŸæ—¶æ‰“å°æ€»ç»“
            print(f"\n>>> Epoch {epoch:2d} å®Œæˆ | "
                  f"å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, å¹³å‡éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
            
            if epoch > 0:
                train_change = avg_train_loss - train_losses[-2]
                val_change = avg_val_loss - val_losses[-2]
                train_trend = "â†“" if train_change < 0 else "â†‘" if train_change > 0 else "="
                val_trend = "â†“" if val_change < 0 else "â†‘" if val_change > 0 else "="
                print(f"    æŸå¤±å˜åŒ–: è®­ç»ƒ {train_trend} {train_change:+.4f}, éªŒè¯ {val_trend} {val_change:+.4f}")
            
            print("-" * 80)
        
        return train_losses, val_losses
    
    def train_step_fast(self, train_data, train_targets, train_batch_indices, val_subset_indices):
        """å¿«é€Ÿè®­ç»ƒæ­¥éª¤ - ä½¿ç”¨éªŒè¯é›†å­é›†"""
        import time
        
        step_start = time.time()
        
        # å®šä¹‰è®­ç»ƒæŸå¤±å‡½æ•°
        def train_loss_func():
            train_loss, train_logit = self.model.train_loss(
                train_data, train_targets, train_batch_indices
            )
            return train_loss, train_logit
        
        # è®¡ç®—éªŒè¯æŸå¤± (ä½¿ç”¨å­é›†)
        val_start = time.time()
        val_loss = self._compute_validation_loss_subset(val_subset_indices)
        val_time = time.time() - val_start
        
        # è¶…å‚æ•°ä¼˜åŒ–æ­¥éª¤
        hyper_start = time.time()
        self.hyper_optimizer.step(train_loss_func, val_loss)
        hyper_time = time.time() - hyper_start
        
        # æ¨¡å‹å‚æ•°ä¼˜åŒ–æ­¥éª¤
        model_start = time.time()
        self.model_optimizer.zero_grad()
        train_loss, _ = train_loss_func()
        train_loss.backward()
        self.model_optimizer.step()
        model_time = time.time() - model_start
        
        total_time = time.time() - step_start
        
        return train_loss.item(), val_loss.item(), {
            'val_time': val_time,
            'hyper_time': hyper_time, 
            'model_time': model_time,
            'total_time': total_time
        }
    
    def train_step_model_only(self, train_data, train_targets, train_batch_indices):
        """åªæ›´æ–°æ¨¡å‹å‚æ•°çš„è®­ç»ƒæ­¥éª¤"""
        import time
        
        step_start = time.time()
        
        # åªè¿›è¡Œæ¨¡å‹å‚æ•°ä¼˜åŒ–
        self.model_optimizer.zero_grad()
        train_loss, _ = self.model.train_loss(train_data, train_targets, train_batch_indices)
        train_loss.backward()
        self.model_optimizer.step()
        
        total_time = time.time() - step_start
        
        # ä½¿ç”¨ä¸Šæ¬¡çš„éªŒè¯æŸå¤±ä½œä¸ºè¿‘ä¼¼ (æˆ–è€…å¯ä»¥è®¾ä¸º0)
        return train_loss.item(), 0.0, {'total_time': total_time}
    
    def _compute_validation_loss_subset(self, subset_indices):
        """è®¡ç®—éªŒè¯é›†å­é›†çš„æŸå¤±"""
        total_loss = 0
        total_samples = 0
        
        # ä»éªŒè¯é›†ä¸­é‡‡æ ·å­é›†
        val_dataset = self.val_loader.dataset
        subset_data = []
        subset_targets = []
        
        for idx in subset_indices:
            data, target = val_dataset[idx]
            subset_data.append(data)
            subset_targets.append(target)
        
        if len(subset_data) > 0:
            val_data = torch.stack(subset_data).to(self.device)
            val_targets = torch.tensor(subset_targets).to(self.device)
            val_loss = self.model.validation_loss(val_data, val_targets)
            return val_loss
        else:
            return torch.tensor(0.0, device=self.device)
    
    def analyze_sample_importance(self, top_k=100):
        """åˆ†ææ ·æœ¬é‡è¦æ€§å¹¶è¿”å›æœ€é‡è¦çš„æ ·æœ¬"""
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è·å–æƒé‡
        if self.model_type == 'original':
            # åŸå§‹æ¨¡å‹ï¼šæ¯ä¸ªæ ·æœ¬æœ‰ç‹¬ç«‹æƒé‡
            weights = self.model.get_sample_weights().detach().cpu().numpy()
        elif self.model_type == 'label_based':
            # åŸºäºæ ‡ç­¾çš„æ¨¡å‹ï¼šæ ¹æ®ç±»åˆ«åˆ†é…æƒé‡
            class_weights = self.model.get_class_weights().detach().cpu().numpy()
            print(f"ğŸ“Š å„æ•°å­—ç±»åˆ«æƒé‡:")
            for i, weight in enumerate(class_weights):
                print(f"   æ•°å­— {i}: {weight:.6f}")
            
            # ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬åˆ†é…å¯¹åº”ç±»åˆ«çš„æƒé‡
            weights = []
            transform = transforms.Compose([transforms.ToTensor()])
            dataset = datasets.MNIST('data', train=True, transform=transform)
            
            for train_idx in self.train_indices:
                _, label = dataset[train_idx]
                weights.append(class_weights[label])
            weights = np.array(weights)
            
        elif self.model_type in ['feature_mlp', 'cnn_feature']:
            # åŸºäºç‰¹å¾çš„æ¨¡å‹ï¼šéœ€è¦éå†æ‰€æœ‰è®­ç»ƒæ ·æœ¬è®¡ç®—æƒé‡
            print("â³ è®¡ç®—æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„æƒé‡ï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
            weights = []
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨æ¥æ‰¹é‡å¤„ç†
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.1307,), (0.3081,))
            ])
            dataset = datasets.MNIST('data', train=True, transform=transform)
            
            batch_size = 256  # è¾ƒå¤§çš„batch sizeä»¥æé«˜æ•ˆç‡
            for i in range(0, len(self.train_indices), batch_size):
                batch_indices = self.train_indices[i:i+batch_size]
                batch_data = []
                
                for idx in batch_indices:
                    data, _ = dataset[idx]
                    batch_data.append(data)
                
                if len(batch_data) > 0:
                    batch_tensor = torch.stack(batch_data).to(self.device)
                    with torch.no_grad():
                        batch_weights = self.model.get_sample_weights(batch_tensor)
                        weights.extend(batch_weights.detach().cpu().numpy())
            
            weights = np.array(weights)
        else:
            raise ValueError(f"Unknown model_type: {self.model_type}")
        
        # æ‰¾åˆ°æƒé‡æœ€é«˜çš„æ ·æœ¬
        top_indices = np.argsort(weights)[-top_k:][::-1]
        top_weights = weights[top_indices]
        
        print(f"\n{'='*60}")
        print(f"æ ·æœ¬é‡è¦æ€§åˆ†æç»“æœ (Top {top_k})")
        print(f"{'='*60}")
        print(f"ğŸ¯ æœ€é«˜æƒé‡æ ·æœ¬: {top_weights[0]:.6f}")
        print(f"ğŸ“Š æƒé‡ç»Ÿè®¡:")
        print(f"   - æœ€ä½æƒé‡: {weights.min():.6f}")
        print(f"   - å¹³å‡æƒé‡: {weights.mean():.6f}")
        print(f"   - æƒé‡æ ‡å‡†å·®: {weights.std():.6f}")
        print(f"   - æƒé‡èŒƒå›´: [{weights.min():.6f}, {weights.max():.6f}]")
        
        # æƒé‡åˆ†å¸ƒåˆ†æ
        high_weight_count = (weights > weights.mean() + weights.std()).sum()
        low_weight_count = (weights < weights.mean() - weights.std()).sum()
        print(f"ğŸ“ˆ æƒé‡åˆ†å¸ƒ:")
        print(f"   - é«˜æƒé‡æ ·æœ¬ (>Î¼+Ïƒ): {high_weight_count} ({high_weight_count/len(weights)*100:.1f}%)")
        print(f"   - ä½æƒé‡æ ·æœ¬ (<Î¼-Ïƒ): {low_weight_count} ({low_weight_count/len(weights)*100:.1f}%)")
        print(f"   - ä¸­ç­‰æƒé‡æ ·æœ¬: {len(weights) - high_weight_count - low_weight_count}")
        
        # Topæ ·æœ¬çš„æƒé‡åˆ†å¸ƒ
        print(f"ğŸ† Top {top_k} æ ·æœ¬æƒé‡èŒƒå›´: [{top_weights[-1]:.6f}, {top_weights[0]:.6f}]")
        print(f"   - Top 10 æƒé‡å¹³å‡: {top_weights[:10].mean():.6f}")
        print(f"   - Top 50 æƒé‡å¹³å‡: {top_weights[:50].mean():.6f}")
        
        return top_indices, top_weights
    
    def visualize_results(self, top_indices, weights, num_samples=20):
        """å¯è§†åŒ–æœ€é‡è¦çš„æ ·æœ¬"""
        
        # åŠ è½½åŸå§‹æ•°æ®é›†ç”¨äºå¯è§†åŒ–
        transform = transforms.Compose([transforms.ToTensor()])
        dataset = datasets.MNIST('data', train=True, transform=transform)
        
        # åˆ†ætopæ ·æœ¬çš„æ•°å­—åˆ†å¸ƒ
        digit_counts = {}
        sample_weights = weights  # ä½¿ç”¨ä¼ å…¥çš„æƒé‡
        
        for i in range(min(num_samples, len(top_indices))):
            real_idx = self.train_indices[top_indices[i]]
            _, label = dataset[real_idx]
            label = int(label)
            if label not in digit_counts:
                digit_counts[label] = 0
            digit_counts[label] += 1
        
        print(f"ğŸ” Top {num_samples} æ ·æœ¬çš„æ•°å­—åˆ†å¸ƒ:")
        for digit in sorted(digit_counts.keys()):
            percentage = (digit_counts[digit] / num_samples) * 100
            print(f"   æ•°å­— {digit}: {digit_counts[digit]:2d} ä¸ªæ ·æœ¬ ({percentage:4.1f}%)")
        
        fig, axes = plt.subplots(4, 5, figsize=(15, 12))
        fig.suptitle(f'å‰{num_samples}ä¸ªæœ€é‡è¦çš„è®­ç»ƒæ ·æœ¬\n(åŸºäºæ•°å­—7çš„éªŒè¯æŸå¤±ä¼˜åŒ–)', fontsize=16)
        
        for i in range(min(num_samples, len(top_indices))):
            row = i // 5
            col = i % 5
            
            # è·å–çœŸå®çš„æ•°æ®é›†ç´¢å¼•
            real_idx = self.train_indices[top_indices[i]]
            image, label = dataset[real_idx]
            weight = sample_weights[top_indices[i]]
            
            axes[row, col].imshow(image.squeeze(), cmap='gray')
            axes[row, col].set_title(f'æ•°å­—: {label}\næƒé‡: {weight:.4f}', fontsize=10)
            axes[row, col].axis('off')
        
        plt.tight_layout()
        plt.show()
        
        return digit_counts


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºMNISTæ•°æ®é€‰æ‹©"""
    
    print("ğŸ¯ MNISTæ•°æ®é€‰æ‹©è¶…å‚æ•°ä¼˜åŒ–")
    print("=" * 60)
    print("ğŸ“‹ ä»»åŠ¡è¯´æ˜:")
    print("   - ä»MNISTæ‰€æœ‰è®­ç»ƒæ ·æœ¬(0-9)ä¸­é€‰æ‹©å¯¹æ•°å­—7è¯†åˆ«æœ€æœ‰å¸®åŠ©çš„æ ·æœ¬")
    print("   - ä½¿ç”¨åŒå±‚ä¼˜åŒ–ï¼šå¤–å±‚å­¦ä¹ æ ·æœ¬æƒé‡ï¼Œå†…å±‚è®­ç»ƒåˆ†ç±»å™¨")
    print("=" * 60)
    
    # 1. è§£æå‚æ•°å’Œåˆå§‹åŒ–
    args = get_args()
    print_model_info(args.selector)
    device = setup_device(args.device)
    
    # 2. åˆ›å»ºæ•°æ®é€‰æ‹©å™¨
    print("\nğŸ“¦ åˆå§‹åŒ–æ•°æ®é€‰æ‹©å™¨...")
    selector = MNISTDataSelector(
        target_digits=[args.target_digit], 
        batch_size=args.batch_size, 
        device=device, 
        model_type=args.selector
    )
    print_training_stats(selector, args.selector)
    
    # 3. è®­ç»ƒæ¨¡å‹
    start_time = time.time()
    train_losses, val_losses = train_model(selector, args)
    print_performance_summary(start_time, train_losses, val_losses, selector)
    
    # 4. åˆ†æå’Œå¯è§†åŒ–
    all_weights = analyze_and_visualize(selector, args)
    plot_results(train_losses, val_losses, all_weights)
    
    # 5. ç”Ÿæˆè¯¦ç»†çš„å›¾ç‰‡åˆ†æ
    try:
        from visualize_results import visualize_mnist_results
        print("\nğŸ–¼ï¸  æ­£åœ¨ç”Ÿæˆè¯¦ç»†çš„å›¾ç‰‡åˆ†æ...")
        visualize_mnist_results(selector, train_losses, val_losses, all_weights, 
                               selector.analyze_sample_importance(top_k=args.top_k)[0])
    except ImportError:
        print("\nâš ï¸  å¯è§†åŒ–æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡è¯¦ç»†åˆ†æ")


if __name__ == "__main__":
    main()


"""
ğŸ“– ä½¿ç”¨ç¤ºä¾‹ï¼š

1. åŸºæœ¬ä½¿ç”¨ (æ¨èçš„label_basedæ¨¡å‹):
   python mnist_data_selection.py --selector label_based --epochs 10 --fast_mode --top_k 1000

2. ä½¿ç”¨ç‰¹å¾MLPæ¨¡å‹:
   python mnist_data_selection.py --selector feature_mlp --epochs 15

3. ä½¿ç”¨CNNç‰¹å¾æ¨¡å‹:
   python mnist_data_selection.py --selector cnn_feature --epochs 20 --fast_mode

4. å®Œæ•´è®­ç»ƒ (è¾ƒæ…¢):
   python mnist_data_selection.py --selector label_based --epochs 30

5. è‡ªå®šä¹‰å‚æ•°:
   python mnist_data_selection.py --selector label_based --epochs 10 --batch_size 128 --top_k 200 --target_digit 9

ğŸ“Š æ€§èƒ½å»ºè®®:
   - åˆå­¦è€…: ä½¿ç”¨ --selector label_based --fast_mode
   - æƒ³è¦æ›´ç²¾ç»†æ§åˆ¶: ä½¿ç”¨ --selector feature_mlp
   - è¿½æ±‚æœ€é«˜æ€§èƒ½: ä½¿ç”¨ --selector cnn_feature (éœ€è¦æ›´å¤šæ—¶é—´)
   - é¿å…ä½¿ç”¨ --selector original (å‚æ•°å¤ªå¤šï¼Œè®­ç»ƒå›°éš¾)

ğŸ¯ é¢„æœŸç»“æœ:
   - label_based: ä¼šå­¦ä¹ åˆ°å“ªäº›æ•°å­—ç±»åˆ«å¯¹è¯†åˆ«æ•°å­—7æœ€æœ‰å¸®åŠ©
   - feature_mlp/cnn_feature: ä¼šå­¦ä¹ åˆ°æ›´ç»†ç²’åº¦çš„æ ·æœ¬é‡è¦æ€§
   - æœ€ç»ˆå¯è§†åŒ–å±•ç¤ºæœ€é‡è¦çš„è®­ç»ƒæ ·æœ¬
"""
