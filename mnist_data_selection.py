"""
MNISTæ•°æ®é€‰æ‹©ç¤ºä¾‹ï¼š
æ ¹æ®æ•°å­—6å’Œ7çš„éªŒè¯æŸå¤±ï¼Œä»0-9çš„è®­ç»ƒæ ·æœ¬ä¸­é€‰æ‹©æœ€ç›¸å…³çš„æ ·æœ¬
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset, Dataset
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt

from model import DataSelectionHyperOptModel
from hyper_opt import NeumannHyperOptimizer, FixedPointHyperOptimizer
from network import SimpleModel


class IndexedDataset(Dataset):
    """å¸¦ç´¢å¼•çš„æ•°æ®é›†ï¼Œç”¨äºè·Ÿè¸ªæ ·æœ¬åœ¨åŸå§‹è®­ç»ƒé›†ä¸­çš„ä½ç½®"""
    def __init__(self, base_dataset, indices):
        self.base_dataset = base_dataset
        self.indices = indices
        
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        real_idx = self.indices[idx]
        data, target = self.base_dataset[real_idx]
        return data, target, idx  # è¿”å›åœ¨train_indicesä¸­çš„ä½ç½®


class MNISTDataSelector:
    def __init__(self, target_digits=[6, 7], batch_size=64, device='cpu'):
        self.target_digits = target_digits
        self.batch_size = batch_size
        self.device = device
        
        # åŠ è½½MNISTæ•°æ®
        self.train_loader, self.val_loader, self.train_indices = self._load_data()
        
        # åˆ›å»ºæ¨¡å‹
        self.network = SimpleModel().to(device)
        self.criterion = nn.CrossEntropyLoss(reduction='none')  # æ³¨æ„è¿™é‡Œéœ€è¦reduction='none'
        
        # åˆ›å»ºæ•°æ®é€‰æ‹©æ¨¡å‹
        num_train_samples = len(self.train_indices)
        self.model = DataSelectionHyperOptModel(
            self.network, 
            self.criterion, 
            num_train_samples
        ).to(device)
        
        # åˆ›å»ºè¶…å‚æ•°ä¼˜åŒ–å™¨
        self.hyper_optimizer = NeumannHyperOptimizer(
            parameters=list(self.model.parameters),
            hyper_parameters=self.model.hyper_parameters,
            base_optimizer='SGD',
            default=dict(lr=0.01),
            use_gauss_newton=True,
            stochastic=True
        )
        # è®¾ç½®è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°ä»¥æé«˜é€Ÿåº¦
        self.hyper_optimizer.set_kwargs(inner_lr=0.1, K=5)  # é»˜è®¤K=20ï¼Œç°åœ¨è®¾ä¸º5
        
        # åˆ›å»ºæ¨¡å‹å‚æ•°ä¼˜åŒ–å™¨
        self.model_optimizer = torch.optim.SGD(self.model.parameters, lr=0.01)
        
    def _load_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†MNISTæ•°æ®"""
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        # åŠ è½½å®Œæ•´æ•°æ®é›†
        train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)
        test_dataset = datasets.MNIST('data', train=False, transform=transform)
        
        # åˆ†ç¦»éªŒè¯é›†ï¼ˆåªåŒ…å«ç›®æ ‡æ•°å­—6å’Œ7ï¼‰
        val_indices = []
        train_indices = []
        
        for idx, (data, target) in enumerate(train_dataset):
            if target in self.target_digits:
                val_indices.append(idx)
            else:
                train_indices.append(idx)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_indexed_dataset = IndexedDataset(train_dataset, train_indices)
        val_subset = Subset(train_dataset, val_indices)
        
        train_loader = DataLoader(train_indexed_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=self.batch_size, shuffle=False)
        
        print(f"è®­ç»ƒæ ·æœ¬æ•°é‡: {len(train_indices)}")
        print(f"éªŒè¯æ ·æœ¬æ•°é‡ (æ•°å­—{self.target_digits}): {len(val_indices)}")
        
        return train_loader, val_loader, train_indices
    
    def train_step(self, train_data, train_targets, train_batch_indices):
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        import time
        
        step_start = time.time()
        
        # å®šä¹‰è®­ç»ƒæŸå¤±å‡½æ•°
        def train_loss_func():
            # é‡æ–°è®¡ç®—è®­ç»ƒæŸå¤±ï¼ˆç”¨äºéšæœºæ¨¡å¼ï¼‰
            train_loss, train_logit = self.model.train_loss(
                train_data, train_targets, train_batch_indices
            )
            return train_loss, train_logit
        
        # è®¡ç®—éªŒè¯æŸå¤± (è¿™é€šå¸¸æ˜¯æœ€æ…¢çš„éƒ¨åˆ†)
        val_start = time.time()
        val_loss = self._compute_validation_loss()
        val_time = time.time() - val_start
        
        # è¶…å‚æ•°ä¼˜åŒ–æ­¥éª¤
        hyper_start = time.time()
        self.hyper_optimizer.step(train_loss_func, val_loss)
        hyper_time = time.time() - hyper_start
        
        # æ¨¡å‹å‚æ•°ä¼˜åŒ–æ­¥éª¤
        model_start = time.time()
        self.model_optimizer.zero_grad()
        train_loss, _ = train_loss_func()
        train_loss.backward()
        self.model_optimizer.step()
        model_time = time.time() - model_start
        
        total_time = time.time() - step_start
        
        # å­˜å‚¨æ—¶é—´ç»Ÿè®¡ç”¨äºåˆ†æ
        if not hasattr(self, 'timing_stats'):
            self.timing_stats = {'val': [], 'hyper': [], 'model': [], 'total': []}
        
        self.timing_stats['val'].append(val_time)
        self.timing_stats['hyper'].append(hyper_time)
        self.timing_stats['model'].append(model_time)
        self.timing_stats['total'].append(total_time)
        
        return train_loss.item(), val_loss.item(), {
            'val_time': val_time,
            'hyper_time': hyper_time, 
            'model_time': model_time,
            'total_time': total_time
        }
    
    def _compute_validation_loss(self):
        """è®¡ç®—éªŒè¯æŸå¤±"""
        total_loss = 0
        total_samples = 0
        
        # ä¸ä½¿ç”¨no_grad()ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦è®¡ç®—éªŒè¯æŸå¤±çš„æ¢¯åº¦
        for val_data, val_targets in self.val_loader:
            val_data, val_targets = val_data.to(self.device), val_targets.to(self.device)
            val_loss = self.model.validation_loss(val_data, val_targets)
            total_loss += val_loss * val_data.size(0)  # ç›´æ¥ä½¿ç”¨tensorï¼Œä¸è½¬æ¢ä¸ºitem
            total_samples += val_data.size(0)
        
        return total_loss / total_samples
    
    def train(self, num_epochs=50):
        """è®­ç»ƒæ•°æ®é€‰æ‹©æ¨¡å‹"""
        train_losses = []
        val_losses = []
        
        print("å¼€å§‹è®­ç»ƒæ•°æ®é€‰æ‹©æ¨¡å‹...")
        print("=" * 80)
        
        for epoch in range(num_epochs):
            epoch_train_loss = 0
            epoch_val_loss = 0
            num_batches = 0
            
            # æ¯ä¸ªepochå¼€å§‹æ—¶æ˜¾ç¤ºæ ·æœ¬æƒé‡ç»Ÿè®¡
            if epoch % 5 == 0:
                weights = self.model.get_sample_weights().detach().cpu().numpy()
                print(f"\nEpoch {epoch} - æ ·æœ¬æƒé‡ç»Ÿè®¡:")
                print(f"  æœ€å¤§æƒé‡: {weights.max():.6f}")
                print(f"  æœ€å°æƒé‡: {weights.min():.6f}")
                print(f"  å¹³å‡æƒé‡: {weights.mean():.6f}")
                print(f"  æƒé‡æ ‡å‡†å·®: {weights.std():.6f}")
                print(f"  é«˜æƒé‡æ ·æœ¬æ•° (>avg): {(weights > weights.mean()).sum()}")
            
            for batch_idx, (train_data, train_targets, train_batch_indices) in enumerate(self.train_loader):
                train_data, train_targets = train_data.to(self.device), train_targets.to(self.device)
                # train_batch_indices å·²ç»ä» IndexedDataset ä¸­è·å¾—
                
                # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
                train_loss, val_loss, timing = self.train_step(
                    train_data, train_targets, train_batch_indices
                )
                
                epoch_train_loss += train_loss
                epoch_val_loss += val_loss
                num_batches += 1
                
                # æ¯20ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
                if batch_idx % 20 == 0:
                    current_weights = self.model.get_sample_weights()[train_batch_indices].detach().cpu().numpy()
                    print(f"  Epoch {epoch:2d}, Batch {batch_idx:3d}/{len(self.train_loader):3d} | "
                          f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} | "
                          f"å½“å‰batchæƒé‡: {current_weights.mean():.4f}Â±{current_weights.std():.4f}")
                    print(f"    â±ï¸  æ—¶é—´åˆ†æ: éªŒè¯æŸå¤± {timing['val_time']:.2f}s, "
                          f"è¶…å‚æ•°ä¼˜åŒ– {timing['hyper_time']:.2f}s, "
                          f"æ¨¡å‹ä¼˜åŒ– {timing['model_time']:.2f}s, "
                          f"æ€»è®¡ {timing['total_time']:.2f}s")
            
            avg_train_loss = epoch_train_loss / num_batches
            avg_val_loss = epoch_val_loss / num_batches
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            # æ¯ä¸ªepochç»“æŸæ—¶æ‰“å°æ€»ç»“
            print(f"\n>>> Epoch {epoch:2d} å®Œæˆ | "
                  f"å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, å¹³å‡éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
            
            # æ˜¾ç¤ºæŸå¤±å˜åŒ–è¶‹åŠ¿
            if epoch > 0:
                train_change = avg_train_loss - train_losses[-2]
                val_change = avg_val_loss - val_losses[-2]
                train_trend = "â†“" if train_change < 0 else "â†‘" if train_change > 0 else "="
                val_trend = "â†“" if val_change < 0 else "â†‘" if val_change > 0 else "="
                print(f"    æŸå¤±å˜åŒ–: è®­ç»ƒ {train_trend} {train_change:+.4f}, éªŒè¯ {val_trend} {val_change:+.4f}")
            
            # æ˜¾ç¤ºè¿™ä¸ªepochçš„å¹³å‡æ—¶é—´ç»Ÿè®¡
            if hasattr(self, 'timing_stats'):
                import numpy as np
                recent_stats = {}
                for key in self.timing_stats:
                    recent_stats[key] = np.mean(self.timing_stats[key][-num_batches:])
                
                print(f"    â±ï¸  å¹³å‡æ—¶é—´åˆ†æ: éªŒè¯æŸå¤± {recent_stats['val']:.2f}s ({recent_stats['val']/recent_stats['total']*100:.1f}%), "
                      f"è¶…å‚æ•°ä¼˜åŒ– {recent_stats['hyper']:.2f}s ({recent_stats['hyper']/recent_stats['total']*100:.1f}%), "
                      f"æ¨¡å‹ä¼˜åŒ– {recent_stats['model']:.2f}s ({recent_stats['model']/recent_stats['total']*100:.1f}%)")
                print(f"    ğŸ“Š æ¯batchå¹³å‡æ—¶é—´: {recent_stats['total']:.2f}s, é¢„è®¡å®Œæˆæ—¶é—´: {recent_stats['total'] * len(self.train_loader) * (num_epochs - epoch - 1) / 60:.1f} åˆ†é’Ÿ")
            
            print("-" * 80)
        
        return train_losses, val_losses
    
    def train_fast(self, num_epochs=50, val_subset_size=1000, hyper_opt_freq=5):
        """å¿«é€Ÿè®­ç»ƒç‰ˆæœ¬ - ä¼˜åŒ–æ€§èƒ½"""
        print("ğŸš€ ä½¿ç”¨å¿«é€Ÿè®­ç»ƒæ¨¡å¼ (æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬)")
        print(f"   - éªŒè¯é›†å­é›†å¤§å°: {val_subset_size}")
        print(f"   - è¶…å‚æ•°ä¼˜åŒ–é¢‘ç‡: æ¯ {hyper_opt_freq} ä¸ªbatch")
        
        # åˆ›å»ºéªŒè¯é›†å­é›†
        val_subset_indices = torch.randperm(len(self.val_loader.dataset))[:val_subset_size]
        
        train_losses = []
        val_losses = []
        
        print("=" * 80)
        
        for epoch in range(num_epochs):
            epoch_train_loss = 0
            epoch_val_loss = 0
            num_batches = 0
            
            # æ¯ä¸ªepochå¼€å§‹æ—¶æ˜¾ç¤ºæ ·æœ¬æƒé‡ç»Ÿè®¡
            if epoch % 5 == 0:
                weights = self.model.get_sample_weights().detach().cpu().numpy()
                print(f"\nEpoch {epoch} - æ ·æœ¬æƒé‡ç»Ÿè®¡:")
                print(f"  æœ€å¤§æƒé‡: {weights.max():.6f}")
                print(f"  æœ€å°æƒé‡: {weights.min():.6f}")
                print(f"  å¹³å‡æƒé‡: {weights.mean():.6f}")
                print(f"  æƒé‡æ ‡å‡†å·®: {weights.std():.6f}")
                print(f"  é«˜æƒé‡æ ·æœ¬æ•° (>avg): {(weights > weights.mean()).sum()}")
            
            for batch_idx, (train_data, train_targets, train_batch_indices) in enumerate(self.train_loader):
                train_data, train_targets = train_data.to(self.device), train_targets.to(self.device)
                
                # åªåœ¨æŒ‡å®šé¢‘ç‡ä¸‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
                if batch_idx % hyper_opt_freq == 0:
                    train_loss, val_loss, timing = self.train_step_fast(
                        train_data, train_targets, train_batch_indices, val_subset_indices
                    )
                else:
                    # åªè¿›è¡Œæ¨¡å‹å‚æ•°ä¼˜åŒ–
                    train_loss, val_loss, timing = self.train_step_model_only(
                        train_data, train_targets, train_batch_indices
                    )
                
                epoch_train_loss += train_loss
                epoch_val_loss += val_loss
                num_batches += 1
                
                # æ¯50ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
                if batch_idx % 50 == 0:
                    current_weights = self.model.get_sample_weights()[train_batch_indices].detach().cpu().numpy()
                    print(f"  Epoch {epoch:2d}, Batch {batch_idx:3d}/{len(self.train_loader):3d} | "
                          f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} | "
                          f"å½“å‰batchæƒé‡: {current_weights.mean():.4f}Â±{current_weights.std():.4f}")
                    if timing:
                        print(f"    â±ï¸  æ—¶é—´: {timing['total_time']:.2f}s")
            
            avg_train_loss = epoch_train_loss / num_batches
            avg_val_loss = epoch_val_loss / num_batches
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            # æ¯ä¸ªepochç»“æŸæ—¶æ‰“å°æ€»ç»“
            print(f"\n>>> Epoch {epoch:2d} å®Œæˆ | "
                  f"å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, å¹³å‡éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
            
            if epoch > 0:
                train_change = avg_train_loss - train_losses[-2]
                val_change = avg_val_loss - val_losses[-2]
                train_trend = "â†“" if train_change < 0 else "â†‘" if train_change > 0 else "="
                val_trend = "â†“" if val_change < 0 else "â†‘" if val_change > 0 else "="
                print(f"    æŸå¤±å˜åŒ–: è®­ç»ƒ {train_trend} {train_change:+.4f}, éªŒè¯ {val_trend} {val_change:+.4f}")
            
            print("-" * 80)
        
        return train_losses, val_losses
    
    def train_step_fast(self, train_data, train_targets, train_batch_indices, val_subset_indices):
        """å¿«é€Ÿè®­ç»ƒæ­¥éª¤ - ä½¿ç”¨éªŒè¯é›†å­é›†"""
        import time
        
        step_start = time.time()
        
        # å®šä¹‰è®­ç»ƒæŸå¤±å‡½æ•°
        def train_loss_func():
            train_loss, train_logit = self.model.train_loss(
                train_data, train_targets, train_batch_indices
            )
            return train_loss, train_logit
        
        # è®¡ç®—éªŒè¯æŸå¤± (ä½¿ç”¨å­é›†)
        val_start = time.time()
        val_loss = self._compute_validation_loss_subset(val_subset_indices)
        val_time = time.time() - val_start
        
        # è¶…å‚æ•°ä¼˜åŒ–æ­¥éª¤
        hyper_start = time.time()
        self.hyper_optimizer.step(train_loss_func, val_loss)
        hyper_time = time.time() - hyper_start
        
        # æ¨¡å‹å‚æ•°ä¼˜åŒ–æ­¥éª¤
        model_start = time.time()
        self.model_optimizer.zero_grad()
        train_loss, _ = train_loss_func()
        train_loss.backward()
        self.model_optimizer.step()
        model_time = time.time() - model_start
        
        total_time = time.time() - step_start
        
        return train_loss.item(), val_loss.item(), {
            'val_time': val_time,
            'hyper_time': hyper_time, 
            'model_time': model_time,
            'total_time': total_time
        }
    
    def train_step_model_only(self, train_data, train_targets, train_batch_indices):
        """åªæ›´æ–°æ¨¡å‹å‚æ•°çš„è®­ç»ƒæ­¥éª¤"""
        import time
        
        step_start = time.time()
        
        # åªè¿›è¡Œæ¨¡å‹å‚æ•°ä¼˜åŒ–
        self.model_optimizer.zero_grad()
        train_loss, _ = self.model.train_loss(train_data, train_targets, train_batch_indices)
        train_loss.backward()
        self.model_optimizer.step()
        
        total_time = time.time() - step_start
        
        # ä½¿ç”¨ä¸Šæ¬¡çš„éªŒè¯æŸå¤±ä½œä¸ºè¿‘ä¼¼ (æˆ–è€…å¯ä»¥è®¾ä¸º0)
        return train_loss.item(), 0.0, {'total_time': total_time}
    
    def _compute_validation_loss_subset(self, subset_indices):
        """è®¡ç®—éªŒè¯é›†å­é›†çš„æŸå¤±"""
        total_loss = 0
        total_samples = 0
        
        # ä»éªŒè¯é›†ä¸­é‡‡æ ·å­é›†
        val_dataset = self.val_loader.dataset
        subset_data = []
        subset_targets = []
        
        for idx in subset_indices:
            data, target = val_dataset[idx]
            subset_data.append(data)
            subset_targets.append(target)
        
        if len(subset_data) > 0:
            val_data = torch.stack(subset_data).to(self.device)
            val_targets = torch.tensor(subset_targets).to(self.device)
            val_loss = self.model.validation_loss(val_data, val_targets)
            return val_loss
        else:
            return torch.tensor(0.0, device=self.device)
    
    def analyze_sample_importance(self, top_k=100):
        """åˆ†ææ ·æœ¬é‡è¦æ€§å¹¶è¿”å›æœ€é‡è¦çš„æ ·æœ¬"""
        
        # è·å–æ‰€æœ‰æ ·æœ¬çš„æƒé‡
        weights = self.model.get_sample_weights().detach().cpu().numpy()
        
        # æ‰¾åˆ°æƒé‡æœ€é«˜çš„æ ·æœ¬
        top_indices = np.argsort(weights)[-top_k:][::-1]
        top_weights = weights[top_indices]
        
        print(f"\n{'='*60}")
        print(f"æ ·æœ¬é‡è¦æ€§åˆ†æç»“æœ (Top {top_k})")
        print(f"{'='*60}")
        print(f"ğŸ¯ æœ€é«˜æƒé‡æ ·æœ¬: {top_weights[0]:.6f}")
        print(f"ğŸ“Š æƒé‡ç»Ÿè®¡:")
        print(f"   - æœ€ä½æƒé‡: {weights.min():.6f}")
        print(f"   - å¹³å‡æƒé‡: {weights.mean():.6f}")
        print(f"   - æƒé‡æ ‡å‡†å·®: {weights.std():.6f}")
        print(f"   - æƒé‡èŒƒå›´: [{weights.min():.6f}, {weights.max():.6f}]")
        
        # æƒé‡åˆ†å¸ƒåˆ†æ
        high_weight_count = (weights > weights.mean() + weights.std()).sum()
        low_weight_count = (weights < weights.mean() - weights.std()).sum()
        print(f"ğŸ“ˆ æƒé‡åˆ†å¸ƒ:")
        print(f"   - é«˜æƒé‡æ ·æœ¬ (>Î¼+Ïƒ): {high_weight_count} ({high_weight_count/len(weights)*100:.1f}%)")
        print(f"   - ä½æƒé‡æ ·æœ¬ (<Î¼-Ïƒ): {low_weight_count} ({low_weight_count/len(weights)*100:.1f}%)")
        print(f"   - ä¸­ç­‰æƒé‡æ ·æœ¬: {len(weights) - high_weight_count - low_weight_count}")
        
        # Topæ ·æœ¬çš„æƒé‡åˆ†å¸ƒ
        print(f"ğŸ† Top {top_k} æ ·æœ¬æƒé‡èŒƒå›´: [{top_weights[-1]:.6f}, {top_weights[0]:.6f}]")
        print(f"   - Top 10 æƒé‡å¹³å‡: {top_weights[:10].mean():.6f}")
        print(f"   - Top 50 æƒé‡å¹³å‡: {top_weights[:50].mean():.6f}")
        
        return top_indices, top_weights
    
    def visualize_results(self, top_indices, num_samples=20):
        """å¯è§†åŒ–æœ€é‡è¦çš„æ ·æœ¬"""
        
        # åŠ è½½åŸå§‹æ•°æ®é›†ç”¨äºå¯è§†åŒ–
        transform = transforms.Compose([transforms.ToTensor()])
        dataset = datasets.MNIST('data', train=True, transform=transform)
        
        # åˆ†ætopæ ·æœ¬çš„æ•°å­—åˆ†å¸ƒ
        digit_counts = {}
        sample_weights = self.model.get_sample_weights().detach().cpu().numpy()
        
        for i in range(min(num_samples, len(top_indices))):
            real_idx = self.train_indices[top_indices[i]]
            _, label = dataset[real_idx]
            label = int(label)
            if label not in digit_counts:
                digit_counts[label] = 0
            digit_counts[label] += 1
        
        print(f"ğŸ” Top {num_samples} æ ·æœ¬çš„æ•°å­—åˆ†å¸ƒ:")
        for digit in sorted(digit_counts.keys()):
            percentage = (digit_counts[digit] / num_samples) * 100
            print(f"   æ•°å­— {digit}: {digit_counts[digit]:2d} ä¸ªæ ·æœ¬ ({percentage:4.1f}%)")
        
        fig, axes = plt.subplots(4, 5, figsize=(15, 12))
        fig.suptitle(f'å‰{num_samples}ä¸ªæœ€é‡è¦çš„è®­ç»ƒæ ·æœ¬\n(åŸºäºæ•°å­—6&7çš„éªŒè¯æŸå¤±ä¼˜åŒ–)', fontsize=16)
        
        for i in range(min(num_samples, len(top_indices))):
            row = i // 5
            col = i % 5
            
            # è·å–çœŸå®çš„æ•°æ®é›†ç´¢å¼•
            real_idx = self.train_indices[top_indices[i]]
            image, label = dataset[real_idx]
            weight = sample_weights[top_indices[i]]
            
            axes[row, col].imshow(image.squeeze(), cmap='gray')
            axes[row, col].set_title(f'æ•°å­—: {label}\næƒé‡: {weight:.4f}', fontsize=10)
            axes[row, col].axis('off')
        
        plt.tight_layout()
        plt.show()
        
        return digit_counts


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºMNISTæ•°æ®é€‰æ‹©"""
    
    print("ğŸ¯ MNISTæ•°æ®é€‰æ‹©è¶…å‚æ•°ä¼˜åŒ–")
    print("=" * 60)
    print("ğŸ“Š æ€§èƒ½åˆ†æè¯´æ˜:")
    print("   - è¶…å‚æ•°ä¼˜åŒ–æ¯”æ™®é€šè®­ç»ƒæ…¢10-50å€")
    print("   - ä¸»è¦ç“¶é¢ˆ: éªŒè¯æŸå¤±è®¡ç®— (~60-80%) + äºŒé˜¶ä¼˜åŒ– (~20-30%)")
    print("   - å¿«é€Ÿæ¨¡å¼å¯æé€Ÿ5-10å€ï¼Œç•¥å¾®é™ä½ç²¾åº¦")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®é€‰æ‹©å™¨
    print("ğŸ“¦ æ­£åœ¨åˆå§‹åŒ–æ•°æ®é€‰æ‹©å™¨...")
    selector = MNISTDataSelector(target_digits=[6, 7], batch_size=64, device=device)
    
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in selector.model.parameters):,}")
    print(f"ğŸ¯ è¶…å‚æ•°æ•°é‡: {sum(p.numel() for p in selector.model.hyper_parameters):,}")
    print(f"âš™ï¸  è®­ç»ƒæ‰¹æ¬¡å¤§å°: {selector.batch_size}")
    print(f"ğŸ”„ æ¯ä¸ªepochæ‰¹æ¬¡æ•°: {len(selector.train_loader)}")
    
    # æ˜¾ç¤ºåˆå§‹æƒé‡ç»Ÿè®¡
    initial_weights = selector.model.get_sample_weights().detach().cpu().numpy()
    print(f"\nğŸ“ˆ åˆå§‹æ ·æœ¬æƒé‡ç»Ÿè®¡:")
    print(f"   å¹³å‡æƒé‡: {initial_weights.mean():.6f}")
    print(f"   æƒé‡æ ‡å‡†å·®: {initial_weights.std():.6f}")
    print(f"   æƒé‡èŒƒå›´: [{initial_weights.min():.6f}, {initial_weights.max():.6f}]")
    
    # è®­ç»ƒæ¨¡å‹
    import time
    start_time = time.time()
    print(f"\nâ° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # é€‰æ‹©è®­ç»ƒæ¨¡å¼
    print(f"\nğŸ”§ è®­ç»ƒé…ç½®:")
    print(f"   1. å®Œæ•´è®­ç»ƒ: æ¯ä¸ªbatchéƒ½è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ– (å‡†ç¡®ä½†æ…¢)")
    print(f"   2. å¿«é€Ÿè®­ç»ƒ: å‡å°‘éªŒè¯é›†å¤§å°å’Œè¶…å‚æ•°ä¼˜åŒ–é¢‘ç‡ (å¿«é€Ÿä½†å¯èƒ½ç¨å·®)")
    
    # è¿™é‡Œé»˜è®¤ä½¿ç”¨å¿«é€Ÿè®­ç»ƒï¼Œä½ å¯ä»¥æ”¹ä¸º selector.train(num_epochs=30) æ¥ä½¿ç”¨å®Œæ•´è®­ç»ƒ
    USE_FAST_TRAINING = True
    
    if USE_FAST_TRAINING:
        train_losses, val_losses = selector.train_fast(
            num_epochs=30, 
            val_subset_size=2000,  # ä»12183ä¸ªæ ·æœ¬å‡å°‘åˆ°2000ä¸ª
            hyper_opt_freq=10      # æ¯10ä¸ªbatchæ‰è¿›è¡Œä¸€æ¬¡è¶…å‚æ•°ä¼˜åŒ–
        )
    else:
        train_losses, val_losses = selector.train(num_epochs=30)
    
    end_time = time.time()
    training_time = end_time - start_time
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {training_time:.1f} ç§’ ({training_time/60:.1f} åˆ†é’Ÿ)")
    print(f"ğŸ“‰ æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}")
    print(f"ğŸ“Š æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.4f}")
    
    # æ€§èƒ½åˆ†ææ€»ç»“
    if hasattr(selector, 'timing_stats') and selector.timing_stats['total']:
        import numpy as np
        avg_batch_time = np.mean(selector.timing_stats['total'])
        avg_val_time = np.mean(selector.timing_stats['val'])
        avg_hyper_time = np.mean(selector.timing_stats['hyper'])
        
        print(f"\nğŸ“ˆ æ€§èƒ½åˆ†ææ€»ç»“:")
        print(f"   å¹³å‡æ¯batchæ—¶é—´: {avg_batch_time:.2f}s")
        print(f"   éªŒè¯æŸå¤±è®¡ç®—: {avg_val_time:.2f}s ({avg_val_time/avg_batch_time*100:.1f}%)")
        print(f"   è¶…å‚æ•°ä¼˜åŒ–: {avg_hyper_time:.2f}s ({avg_hyper_time/avg_batch_time*100:.1f}%)")
        
        estimated_fast_time = training_time * 0.1  # å¿«é€Ÿæ¨¡å¼ä¼°è®¡æé€Ÿ10å€
        print(f"   ä¼°è®¡å¿«é€Ÿæ¨¡å¼æ—¶é—´: {estimated_fast_time/60:.1f} åˆ†é’Ÿ (æé€Ÿçº¦10å€)")
    
    print("\nğŸ’¡ è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®:")
    print("   1. å‡å°‘éªŒè¯é›†å¤§å°: val_subset_size=500-1000")
    print("   2. é™ä½è¶…å‚æ•°ä¼˜åŒ–é¢‘ç‡: hyper_opt_freq=20-50")  
    print("   3. å‡å°‘Neumannè¿­ä»£æ¬¡æ•°: K=3-5")
    print("   4. ä½¿ç”¨æ›´ç®€å•çš„ä¼˜åŒ–å™¨: use_gauss_newton=False")
    
    # åˆ†ææ ·æœ¬é‡è¦æ€§
    top_indices, top_weights = selector.analyze_sample_importance(top_k=100)
    
    # å¯è§†åŒ–ç»“æœ
    print(f"\nğŸ¨ æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    selector.visualize_results(top_indices, num_samples=20)
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±')
    plt.plot(val_losses, label='éªŒè¯æŸå¤± (6&7)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('è®­ç»ƒè¿‡ç¨‹')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.hist(selector.model.get_sample_weights().detach().cpu().numpy(), bins=50, alpha=0.7)
    plt.xlabel('æ ·æœ¬æƒé‡')
    plt.ylabel('é¢‘æ•°')
    plt.title('æ ·æœ¬æƒé‡åˆ†å¸ƒ')
    
    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    main()
